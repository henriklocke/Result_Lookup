{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bfa4f1",
   "metadata": {},
   "source": [
    "##Info\n",
    "<!-- \n",
    "\n",
    "To run this notebook, click menu Cell -> Run All\n",
    "\n",
    "Workflow:\n",
    "    1. Sum:\n",
    "        WW\n",
    "        GWI\n",
    "        BSF\n",
    "        Runoff\n",
    "        dfs0 inflow\n",
    "    2. Sum:\n",
    "        WWTP flow\n",
    "        MH Spilling\n",
    "        Outfalls\n",
    "        Delta volume\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb6340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 1\n",
    "\n",
    "import os\n",
    "import mikeio\n",
    "import mikeio1d\n",
    "from mikeio1d.res1d import Res1D\n",
    "from mikeio.dfs0 import Dfs0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ctypes\n",
    "import traceback\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA\n",
    "from Result_Lookup_Variables import *\n",
    "import subprocess\n",
    "import sqlite3\n",
    "from datetime import datetime as dt, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198888cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql function\n",
    "def sql_to_df(sql,model):\n",
    "  con = sqlite3.connect(model)\n",
    "  df = pd.read_sql(sql, con)\n",
    "  con.close()\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a5691a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Nov 15 2021, 2060 Pop\n",
      "Import DWF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "flood_types = ['WaterFlowRateAboveGround','WaterSpillDischarge']\n",
    "cover_types = ['Normal','Spilling']\n",
    "# boundary_inflows = []\n",
    "# boundary_inflows.append('Landfill')\n",
    "\n",
    "for m in master_list:\n",
    "  \n",
    "    model_area = m[0]\n",
    "    model = m[1]\n",
    "    result_folder = m[2]\n",
    "    output_folder = m[3]\n",
    "    result_list = m[4]\n",
    "    groupby_acronym_owner = m[5]\n",
    "    element_filter = m[7]\n",
    "    wwtp_pipe = m[9]\n",
    "    outfall_summary = m[10]\n",
    "    \n",
    "    outfall_disconnection_df = pd.read_csv(outfall_summary)\n",
    "    \n",
    "    #find subfolders for MIKE+\n",
    "    if model[-7:] == '.sqlite':\n",
    "        result_dict = {}\n",
    "        not_founds = []\n",
    "        for r in result_list:\n",
    "            file = r[1]\n",
    "            file_found = False\n",
    "            for f1 in os.listdir(result_folder):\n",
    "                if f1[-7:] == '.sqlite':\n",
    "                    #browse subfolder\n",
    "                    result_subfolder = os.path.basename(f1)[:-7] + '_m1d - Result Files'\n",
    "                    try:\n",
    "                        for f2 in os.listdir(result_folder + '\\\\' + result_subfolder):\n",
    "                            if os.path.basename(f2) == file:\n",
    "                                result_dict[file] = [f1,'\\\\' + result_subfolder]\n",
    "                                file_found = True\n",
    "                    except:\n",
    "                        pass\n",
    "            if not file_found:\n",
    "                not_founds.append(file)\n",
    " \n",
    "    output_subfolder = output_folder + '\\\\All_Mass_Balances'\n",
    "    if not os.path.isdir(output_subfolder): os.makedirs(output_subfolder) \n",
    "\n",
    "    html_path = output_subfolder + '\\\\Mass_Balance.html'\n",
    "    f = open(html_path, \"w\")\n",
    "    f.write('<link rel=\"stylesheet\" href=\"..\\Maps_And_CSS\\style_mb.css\">\\n')\n",
    "    f.write('<script src=\"..\\Maps_And_CSS\\script_mb.js\"></script>\\n')\n",
    "    f.write('<!DOCTYPE html>\\n')\n",
    "    f.write('<html>\\n')\n",
    "    f.write('<head>\\n')\n",
    "    f.write('<meta charset=\"utf-8\">\\n')\n",
    "    f.write('</head>\\n')\n",
    "    f.write('<body>\\n\\n')\n",
    "\n",
    "    f.write('<div class=\"tab\">\\n')\n",
    "    for r in result_list:\n",
    "        tab = r[0]       \n",
    "        f.write('  <button class=\"tablinks\" onclick=\"openTab(event, ' + \"'\" + tab + \"'\"  + ')\">' + tab + '</button>\\n')\n",
    "    f.write('</div>\\n')\n",
    "       \n",
    "    \n",
    "    for r in result_list:\n",
    "        \n",
    "        table_specs = []\n",
    "        table_specs.append(['Inflow and Outflow',['Total Inflow','Total Outflow']])\n",
    "        table_specs.append(['Inflow Breakdown',['Runoff','GWI','Wastewater','DWF','BSF','Boundary']])\n",
    "        table_specs.append(['Outflow Breakdown',['WWTP','Disconnection','Overflow','Spill']])\n",
    "        \n",
    "        header = r[0]\n",
    "        tab = header\n",
    "        file = r[1]\n",
    "        model_path = result_folder + '\\\\' + result_dict[file][0]\n",
    "        result_network_path = result_folder + '\\\\' + result_dict[file][1] + '\\\\' + file\n",
    "        result_runoff_path = result_network_path[:-16] + 'Surface_runoff.res1d'\n",
    "        \n",
    "        individual_dfs = []\n",
    "        \n",
    "        print('Importing ' + header)\n",
    "        \n",
    "        f.write('<div id=\"' + tab + '\" class=\"tabcontent\">\\n')  \n",
    "        f.write('<h1>' + tab + '</h1>')\n",
    "        \n",
    "        f.write('<div class=\"row\"><div class=\"column\">\\n')\n",
    "       \n",
    "        print('Import DWF')\n",
    "        sql = \"SELECT ms_DPProfileD.ScheduleID AS Day_Type, ms_DPPatternD.Sqn AS [Hour], Sum(msm_Loadpoint.loadflow*ms_DPPatternD.DPValue) AS Wastewater \"\n",
    "        sql += \"FROM ((msm_Loadpoint INNER JOIN msm_BBoundary ON msm_Loadpoint.LoadCategoryNo = msm_BBoundary.LoadCategoryNo) INNER JOIN ms_DPProfileD ON msm_BBoundary.DPProfileID = ms_DPProfileD.ProfileID) INNER JOIN ms_DPPatternD ON ms_DPProfileD.PatternID = ms_DPPatternD.PatternID \"\n",
    "        sql += \"WHERE msm_Loadpoint.Active = 1 AND ms_DPProfileD.Active = 1 AND ms_DPPatternD.Active = 1 AND msm_BBoundary.Active = 1 \"\n",
    "        sql += \"GROUP BY ms_DPProfileD.ScheduleID, ms_DPPatternD.Time \"\n",
    "        sql += \"HAVING (LOWER(SUBSTR(ms_DPProfileD.ScheduleID,1,7))='weekday' Or LOWER(SUBSTR(ms_DPProfileD.ScheduleID,1,7))='weekend') AND ms_DPPatternD.Sqn <> 0 \"\n",
    "        sql += \"ORDER BY scheduleid, time\"\n",
    "        diurnal_wws = sql_to_df(sql,model_path) \n",
    "        \n",
    "        sql = \"SELECT SUM(loadflow) FROM msm_Loadpoint WHERE description = 'Baseflow' and Active = 1\"\n",
    "        gwi = sql_to_df(sql,model_path).iloc[0,0]\n",
    "        \n",
    "        diurnal_wws.Hour = diurnal_wws.Hour - 1\n",
    "#         diurnal_wws.Wastewater = diurnal_wws.Wastewater\n",
    "        \n",
    "        sql = \"SELECT COUNT(muid) FROM msm_Loadpoint WHERE description = 'BSF' and Active = 1\"\n",
    "        bsf_count = sql_to_df(sql,model_path).iloc[0,0]\n",
    "        bsf = 0\n",
    "        bsf_on = True if bsf_count > 0 else False\n",
    "        if bsf_on:\n",
    "            print('Import BSF')\n",
    "            sql = \"SELECT SUM(loadflow) FROM msm_Loadpoint WHERE description = 'BSF' and Active = 1\"\n",
    "            bsf = sql_to_df(sql,model_path).iloc[0,0]\n",
    "            sql = \"SELECT SUM(area) FROM msm_Catchment WHERE Active = 1\"\n",
    "            area = sql_to_df(sql,model_path).iloc[0,0]\n",
    "            ini_rate = round(bsf * 86400 / area * 10000 * 1000, 0)\n",
    "            ini_no = ini_rate / 11200 \n",
    "            \n",
    "        #Extract results\n",
    "        print('Import WWTP')\n",
    "        res1d = Res1D(result_network_path)\n",
    "        sim_start = res1d.time_index.min()\n",
    "        start = sim_start + timedelta(days=1)\n",
    "        end = res1d.time_index.max()\n",
    "        sim_seconds = (end - sim_start).total_seconds()\n",
    "        timesteps = len(res1d.time_index)-1\n",
    "        timestep_seconds = sim_seconds / timesteps\n",
    "        skip_steps = int(86400 / timestep_seconds)\n",
    "        \n",
    "        wwtp_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "        wwtp_df['WWTP'] = list(res1d.query.GetReachEndValues(wwtp_pipe, \"Discharge\"))[skip_steps:]\n",
    "                \n",
    "        print('Import Disconnections')\n",
    "        disc_df = outfall_disconnection_df[outfall_disconnection_df.Type=='Disconnection']\n",
    "        first_round = True\n",
    "        has_disc = False\n",
    "        for index, row in disc_df.iterrows():\n",
    "            scenario = row['Scenario']\n",
    "            if scenario in file:\n",
    "                muid = row['Structure']\n",
    "                layer = row['Layer']\n",
    "                outfall = row['Outfall']\n",
    "                resid = muid\n",
    "                if layer.lower() != 'msm_link':\n",
    "                    resid = layer[4:] + ':' + muid\n",
    "\n",
    "                disc_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "                ts = list(res1d.query.GetReachEndValues(resid, \"Discharge\"))[skip_steps:]\n",
    "                disc_df['Outfall'] = outfall\n",
    "                disc_df['Disconnection'] = ts\n",
    "                if first_round == True:\n",
    "                    disc_df_all = disc_df.copy()\n",
    "                else:\n",
    "                    disc_df_all = pd.concat([disc_df_all,disc_df])                                                        \n",
    "                first_round = False\n",
    "                has_disc = True\n",
    "                \n",
    "        if has_disc:\n",
    "            disconnection_df = disc_df_all.pivot(columns='Outfall', values='Disconnection')\n",
    "            individual_dfs.append(['Disconnections',disconnection_df])\n",
    "            disc_df_all = disc_df_all.groupby(disc_df_all.index).agg({'Disconnection': 'sum'}) \n",
    "        \n",
    "        print('Import outfalls')\n",
    "        outfall_df = outfall_disconnection_df[outfall_disconnection_df.Type=='Overflow'][['Structure', 'Layer', 'Outfall']]\n",
    "        outfall_df.sort_values(by=['Outfall','Structure'],inplace=True)\n",
    "        outfall_df.reset_index(drop=True,inplace=True)\n",
    "        first_round = True\n",
    "        has_overflow = False\n",
    "        \n",
    "        overflow_by_structure_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "        \n",
    "        for index, row in outfall_df.iterrows():\n",
    "            \n",
    "            muid = row['Structure']\n",
    "            layer = row['Layer']\n",
    "            outfall = row['Outfall']\n",
    "            resid = muid\n",
    "            if layer.lower() != 'msm_link':\n",
    "                resid = layer[4:] + ':' + muid\n",
    "\n",
    "            overflow_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "            ts = list(res1d.query.GetReachEndValues(resid, \"Discharge\"))[skip_steps:]\n",
    "            overflow_df['MUID'] = muid\n",
    "            overflow_df['Outfall'] = outfall\n",
    "            overflow_df['Overflow'] = ts\n",
    "            \n",
    "            overflow_by_structure_df[muid + ' (to ' + outfall + ')'] = ts\n",
    "            \n",
    "            if first_round == True:\n",
    "                overflow_df_all = overflow_df.copy()\n",
    "            else:\n",
    "                overflow_df_all = pd.concat([overflow_df_all,overflow_df])                                                        \n",
    "            first_round = False\n",
    "            has_overflow = True\n",
    "                \n",
    "        if has_overflow:       \n",
    "            overflow_by_outfall_df = overflow_df_all.groupby([overflow_df_all.index,overflow_df_all.Outfall]).agg({'Overflow': 'sum'}) \n",
    "            overflow_by_outfall_df.reset_index(level='Outfall', inplace=True)\n",
    "            overflow_by_outfall_df = overflow_by_outfall_df.pivot(columns='Outfall', values='Overflow')\n",
    "            overflow_df_all = overflow_df_all.groupby(overflow_df_all.index).agg({'Overflow': 'sum'})\n",
    "            individual_dfs.append(['Outfalls',overflow_by_outfall_df])\n",
    "            individual_dfs.append(['Overflow Structures',overflow_by_structure_df])\n",
    "\n",
    "        has_boundary = False\n",
    "        print('Import inflow')\n",
    "        sql = \"SELECT tsconnection, timeseriesname FROM msm_BBoundary WHERE active = 1 AND typeno = 9 \"\n",
    "        sql += \"AND variationno = 3\"\n",
    "        df = sql_to_df(sql,model_path)\n",
    "        first_round = True\n",
    "        for index, row in df.iterrows():\n",
    "            has_boundary= True\n",
    "            rel_path = row['tsconnection']\n",
    "            timeseriesname = row['timeseriesname']\n",
    "            dfs0_path = os.path.abspath(os.path.join(result_folder, rel_path))\n",
    "            res = mikeio.read(dfs0_path)\n",
    "            inflow_df = res.to_dataframe()\n",
    "            for i, col in enumerate(inflow_df.columns):\n",
    "                if col == timeseriesname:\n",
    "                    col_no = i\n",
    "            inflow_df = inflow_df[[timeseriesname]]\n",
    "            inflow_df['Boundary'] = timeseriesname\n",
    "            inflow_df.rename(columns={timeseriesname:'Inflow'},inplace=True)\n",
    "            inflow_df = inflow_df[['Boundary','Inflow']]\n",
    "            \n",
    "            if '(liter per sec)' in str(res.items[col_no]):\n",
    "                inflow_df.Inflow = inflow_df.Inflow/1000\n",
    "                \n",
    "            if first_round == True:\n",
    "                inflow_df_all = inflow_df.copy()\n",
    "            else:\n",
    "                inflow_df_all = pd.concat([inflow_df_all,inflow_df])                                                        \n",
    "            first_round = False\n",
    "        \n",
    "        if has_boundary:\n",
    "            inflow_by_boundary_df = inflow_df_all.pivot(columns='Boundary', values='Inflow')\n",
    "            inflow_by_boundary_df = inflow_by_boundary_df.loc[start:end]\n",
    "            individual_dfs.append(['Boundaries',inflow_by_boundary_df])\n",
    "            inflow_df_all = inflow_df_all.groupby(inflow_df_all.index).agg({'Inflow': 'sum'}) \n",
    "            inflow_df_all.rename(columns={'Inflow':'Boundary'},inplace=True)   \n",
    "            \n",
    "        print('Import spill')\n",
    "        has_spill = False\n",
    "        first_round = True\n",
    "        for node in res1d.data.Nodes:\n",
    "            muid = node.Id\n",
    "            for i, flood_type in enumerate(flood_types):\n",
    "                ts = res1d.query.GetNodeValues(muid,flood_type)\n",
    "                if ts != None:\n",
    "                    if max(ts) > 0:\n",
    "                        spill_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "                        spill_df['Node'] = muid\n",
    "                        spill_df['Spill'] = list(ts)[skip_steps:]\n",
    "                        if first_round == True:\n",
    "                            spill_df_all = spill_df.copy()\n",
    "                        else:\n",
    "                            spill_df_all = pd.concat([spill_df_all,spill_df])                                                        \n",
    "                        first_round = False\n",
    "                        has_spill = True\n",
    "        \n",
    "        if has_spill:\n",
    "            spill_by_node_df = spill_df_all.pivot(columns='Node', values='Spill')\n",
    "            ordered_cols = [muid for muid in spill_by_node_df.sum().sort_values(ascending=False).index]\n",
    "            spill_by_node_df = spill_by_node_df[ordered_cols]\n",
    "            individual_dfs.append(['Spills',spill_by_node_df])\n",
    "            spill_df_all = spill_df_all.groupby(spill_df_all.index).agg({'Spill': 'sum'})\n",
    "            \n",
    "        print('Import runoff')\n",
    "        has_runoff = False\n",
    "        first_round = True\n",
    "        if os.path.exists(result_runoff_path):\n",
    "            res1d = Res1D(result_runoff_path)\n",
    "            has_runoff = True\n",
    "            for i, catchment in enumerate(res1d.data.Catchments):\n",
    "                ts_id = catchment.Id\n",
    "                if not ' - RDI' in ts_id and not ' - Kinematic wave (B)' in ts_id:\n",
    "#                     print('Importing catchment ' + str((i+1)/3) + ' of ' + str(len(res1d.data.Catchments)/3) + ': ' + muid)\n",
    "                    muid = ts_id\n",
    "                    ts = res1d.query.GetCatchmentValues(muid,'TotalRunOff')\n",
    "                    runoff_df = pd.DataFrame(index=res1d.time_index)\n",
    "                    runoff_df['Node'] = muid\n",
    "                    runoff_df['Runoff'] = ts\n",
    "                    if first_round == True:\n",
    "                        runoff_df_all = runoff_df.copy()\n",
    "                    else:\n",
    "                        runoff_df_all = pd.concat([runoff_df_all,runoff_df])                                                        \n",
    "                    first_round = False\n",
    "                    \n",
    "                           \n",
    "            runoff_df_all = runoff_df_all.groupby(runoff_df_all.index).agg({'Runoff': 'sum'})\n",
    "            \n",
    "         \n",
    "                                \n",
    "        df_all = wwtp_df.copy()\n",
    "        if has_overflow:\n",
    "            df_all = pd.merge(df_all, overflow_df_all, left_index=True, right_index=True, how='left')\n",
    "        else:\n",
    "            df_all['Overflow'] = 0\n",
    "        if has_disc:\n",
    "            df_all = pd.merge(df_all, disc_df_all, left_index=True, right_index=True, how='left')\n",
    "        else:\n",
    "            df_all['Disconnection'] = 0\n",
    "        if has_spill:\n",
    "            df_all = pd.merge(df_all, spill_df_all, left_index=True, right_index=True, how='left')\n",
    "        else:\n",
    "            df_all['Spill'] = 0\n",
    "        if has_boundary:\n",
    "            df_all = pd.merge(df_all, inflow_df_all, left_index=True, right_index=True, how='left')\n",
    "        else:\n",
    "            df_all['Boundary'] = 0\n",
    "        df_all['Total Outflow'] = df_all.WWTP + df_all.Spill + df_all.Overflow + df_all.Disconnection\n",
    "        if has_runoff:\n",
    "            df_all = pd.merge(df_all, runoff_df_all, left_index=True, right_index=True, how='left')\n",
    "        else:\n",
    "            df_all['Runoff'] = 0\n",
    "        df_all['DateTime'] = df_all.index\n",
    "        df_all['Hour'] = df_all.DateTime.dt.hour\n",
    "        df_all['Weekday'] = df_all['DateTime'].dt.day_name()\n",
    "        df_all['Day_Type'] = 'Weekdays'\n",
    "        df_all.loc[df_all['Weekday']=='Saturday','Day_Type']='Weekends'\n",
    "        df_all.loc[df_all['Weekday']=='Sunday','Day_Type']='Weekends'\n",
    "        df_all = pd.merge(df_all,diurnal_wws[['Day_Type', 'Hour','Wastewater']],on=['Day_Type', 'Hour'],how='inner')\n",
    "        df_all.set_index('DateTime',inplace=True)\n",
    "        df_all.sort_index(inplace=True)\n",
    "        df_all['Wastewater'] = df_all['Wastewater'].rolling('1h').mean()\n",
    "        df_all.fillna(method='bfill',inplace=True)\n",
    "        df_all.drop(columns=['Hour','Weekday','Day_Type'],inplace=True)\n",
    "        df_all['GWI'] = gwi\n",
    "        df_all['DWF'] = df_all['GWI'] + df_all['Wastewater']\n",
    "        df_all['BSF'] = bsf\n",
    "        df_all['Total Inflow'] = df_all.Boundary + df_all.Runoff + df_all.DWF + df_all.BSF\n",
    "        \n",
    "        maxes = df_all.max()\n",
    "        sums = df_all.sum()\n",
    "        \n",
    "        for individual_df in individual_dfs:  \n",
    "            table_specs.append([individual_df[0],list(individual_df[1].columns)])\n",
    "            maxes = pd.concat([maxes,individual_df[1].max()])\n",
    "            sums = pd.concat([sums,individual_df[1].sum()])\n",
    "        \n",
    "        for table_spec in table_specs:\n",
    "            items = table_spec[1]\n",
    "            f.write('<h2>' + table_spec[0] + '</h2>\\n')\n",
    "            f.write('<table>\\n')\n",
    "            f.write('<tr>\\n')\n",
    "            f.write('<th rowspan=\"2\">Description</th>\\n') \n",
    "            f.write('<th style=\"text-align: center\">Volume</th>\\n') \n",
    "            f.write('<th style=\"text-align: center\">Peak Flow</th>\\n') \n",
    "            f.write('</tr>\\n')\n",
    "            f.write('<tr>\\n')\n",
    "            f.write('<th style=\"text-align: center\">ML</th>\\n')\n",
    "            f.write('<th style=\"text-align: center\">L/s</th>\\n') \n",
    "            f.write('</tr>\\n')\n",
    "            \n",
    "            for item in items:\n",
    "                \n",
    "                f.write('<tr>\\n')\n",
    "                f.write('<td style=\"text-align: left\">' + item + '</td>\\n')\n",
    "                volume = int(sums[item]*timestep_seconds/1000) if not np.isnan(sums[item]) else 0\n",
    "                f.write('<td style=\"text-align: right\">'+ str(volume) + '</td>\\n')\n",
    "                flow = int(maxes[item]*1000) if not np.isnan(maxes[item]) else 0\n",
    "                f.write('<td style=\"text-align: right\">'+ str(flow) + '</td>\\n')\n",
    "\n",
    "                f.write('</tr>\\n')\n",
    "            f.write('</table>\\n')\n",
    "           \n",
    "        f.write('</div>\\n')             \n",
    "        f.write('<div class=\"column\">\\n')\n",
    "        \n",
    "        grouped_cols = [list(individual_df[1].columns) for individual_df in individual_dfs]\n",
    "        cols = [col for sublist in grouped_cols for col in sublist]\n",
    "        \n",
    "        all_cols = list(df_all.columns) + cols       \n",
    "\n",
    "        buttons_ons = []\n",
    "        buttons_ons.append(['Inflow and Outflow',['Total Inflow','Total Outflow']])\n",
    "        buttons_ons.append(['Inflows',['Runoff','GWI','Wastewater','DWF','Boundary','BSF','Total Inflow']])\n",
    "        buttons_ons.append(['Outflows',['WWTP','Disconnection','Overflow','Spill','Total Outflow']])\n",
    "        \n",
    "        for individual_df in individual_dfs:\n",
    "            stophere = 9 if individual_df[0] == 'Spills' else len(individual_df[1].columns)\n",
    "            button_name = individual_df[0] + ' Top 10' if individual_df[0] == 'Spills' else individual_df[0]\n",
    "            buttons_ons.append([button_name,list(individual_df[1].columns[:stophere])])\n",
    "\n",
    "#         falses = [False for i in range(sum([len(individual_df[1].columns) for individual_df in individual_dfs]))]\n",
    "#         individuals = len(falses)\n",
    "        for button_ons in buttons_ons:\n",
    "            on_list = []\n",
    "            for col in all_cols:\n",
    "                on_list.append(col in button_ons[1])\n",
    "            button_ons[1] = on_list\n",
    "#             button_ons[1] += falses\n",
    "        buttons_ons\n",
    "\n",
    "\n",
    "        fig = go.Figure()\n",
    "        for col in df_all.columns:   \n",
    "\n",
    "            is_visible = True if col in ['Total Inflow','Total Outflow'] else False\n",
    "            fig.add_trace(go.Scatter(x=df_all.index, \n",
    "                                             y = df_all[col], \n",
    "                                             mode='lines',name=col, visible=is_visible))\n",
    "        for individual_df in individual_dfs:\n",
    "            stophere = 9 if individual_df[0] == 'Spills' else len(individual_df[1].columns)\n",
    "            \n",
    "            for col in individual_df[1].columns[:stophere]:\n",
    "                fig.add_trace(go.Scatter(x=individual_df[1].index, \n",
    "                                             y = individual_df[1][col], \n",
    "                                             mode='lines',name=col, visible=False, showlegend=True ))\n",
    "\n",
    "\n",
    "        fig.update_layout(\n",
    "            title = 'Model inflows and outflows',\n",
    "            autosize=False,\n",
    "            width = 1362,\n",
    "            height=700,\n",
    "            margin=dict(\n",
    "                l=50,\n",
    "                r=50,\n",
    "                b=25,\n",
    "                t=35,\n",
    "                pad=4\n",
    "                ),\n",
    "            yaxis_title='Discharge (cms)',                        \n",
    "\n",
    "            updatemenus=[\n",
    "                {\n",
    "                    'buttons': [\n",
    "                        {\n",
    "                            'args': [{'visible': button_on[1]}, {'title': button_on[0]}],\n",
    "                            'label': button_on[0],\n",
    "                            'method': 'update'\n",
    "                        }\n",
    "                        for button_on in buttons_ons\n",
    "                    ],\n",
    "                    'direction': 'left',\n",
    "                    'pad': {'r': 10, 't': 87},\n",
    "                    'showactive': True,\n",
    "                    'type': 'buttons',\n",
    "                    'x': 0.1,\n",
    "                    'xanchor': 'left',\n",
    "                    'y': 0.06,\n",
    "                    'yanchor': 'top'\n",
    "                }\n",
    "                ]                                           \n",
    "            )\n",
    "#         fig['layout']['yaxis']['range']=[0:]\n",
    "\n",
    "        f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "        f.write('</div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('</div>\\n')\n",
    "\n",
    "    f.write('</body>\\n')\n",
    "    f.write('</html>\\n')\n",
    "    f.close()\n",
    "        \n",
    "        \n",
    "print('Done')                \n",
    "                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b90bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_mike",
   "language": "python",
   "name": "py39_mike"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
