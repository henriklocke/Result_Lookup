{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bfa4f1",
   "metadata": {},
   "source": [
    "##Info\n",
    "<!-- \n",
    "\n",
    "To run this notebook, click menu Cell -> Run All\n",
    "\n",
    "Workflow:\n",
    "    1. Sum:\n",
    "        WW\n",
    "        GWI\n",
    "        BSF\n",
    "        Runoff\n",
    "        dfs0 inflow\n",
    "    2. Sum:\n",
    "        WWTP flow\n",
    "        MH Spilling\n",
    "        Outfalls\n",
    "        Delta volume\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 1\n",
    "\n",
    "import os\n",
    "import mikeio\n",
    "import mikeio1d\n",
    "from mikeio1d.res1d import Res1D\n",
    "from mikeio.dfs0 import Dfs0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ctypes\n",
    "import traceback\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA\n",
    "from Result_Lookup_Variables import *\n",
    "import subprocess\n",
    "import sqlite3\n",
    "from datetime import datetime as dt, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198888cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql function\n",
    "def sql_to_df(sql,model):\n",
    "  con = sqlite3.connect(model)\n",
    "  df = pd.read_sql(sql, con)\n",
    "  con.close()\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5691a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flood_types = ['WaterFlowRateAboveGround','WaterSpillDischarge']\n",
    "cover_types = ['Normal','Spilling']\n",
    "boundary_inflows = []\n",
    "boundary_inflows.append('Landfill')\n",
    "\n",
    "for m in master_list:\n",
    "  \n",
    "    model_area = m[0]\n",
    "    model = m[1]\n",
    "    result_folder = m[2]\n",
    "    output_folder = m[3]\n",
    "    result_list = m[4]\n",
    "    groupby_acronym_owner = m[5]\n",
    "    element_filter = m[7]\n",
    "    wwtp_pipe = m[9]\n",
    "    outfall_summary = m[10] \n",
    "    \n",
    "    #find subfolders for MIKE+\n",
    "    if model[-7:] == '.sqlite':\n",
    "        result_dict = {}\n",
    "        not_founds = []\n",
    "        for r in result_list:\n",
    "            file = r[1]\n",
    "            file_found = False\n",
    "            for f1 in os.listdir(result_folder):\n",
    "                if f1[-7:] == '.sqlite':\n",
    "                    #browse subfolder\n",
    "                    result_subfolder = os.path.basename(f1)[:-7] + '_m1d - Result Files'\n",
    "                    try:\n",
    "                        for f2 in os.listdir(result_folder + '\\\\' + result_subfolder):\n",
    "                            if os.path.basename(f2) == file:\n",
    "                                result_dict[file] = [f1,'\\\\' + result_subfolder]\n",
    "                                file_found = True\n",
    "                    except:\n",
    "                        pass\n",
    "            if not file_found:\n",
    "                not_founds.append(file)\n",
    "    \n",
    "    for r in result_list[:1]:\n",
    "        header = r[0]\n",
    "        file = r[1]\n",
    "        model_path = result_folder + '\\\\' + result_dict[file][0]\n",
    "        result_network_path = result_folder + '\\\\' + result_dict[file][1] + '\\\\' + file\n",
    "        result_runoff_path = result_network_path[:-16] + 'Surface_runoff.res1d'\n",
    "\n",
    "        \n",
    "        print('Import DWF')\n",
    "        sql = \"SELECT ms_DPProfileD.ScheduleID AS Day_Type, ms_DPPatternD.Sqn AS [Hour], Sum(msm_Loadpoint.loadflow*ms_DPPatternD.DPValue) AS DWF \"\n",
    "        sql += \"FROM ((msm_Loadpoint INNER JOIN msm_BBoundary ON msm_Loadpoint.LoadCategoryNo = msm_BBoundary.LoadCategoryNo) INNER JOIN ms_DPProfileD ON msm_BBoundary.DPProfileID = ms_DPProfileD.ProfileID) INNER JOIN ms_DPPatternD ON ms_DPProfileD.PatternID = ms_DPPatternD.PatternID \"\n",
    "        sql += \"WHERE msm_Loadpoint.Active = 1 AND ms_DPProfileD.Active = 1 AND ms_DPPatternD.Active = 1 AND msm_BBoundary.Active = 1 \"\n",
    "        sql += \"GROUP BY ms_DPProfileD.ScheduleID, ms_DPPatternD.Time \"\n",
    "        sql += \"HAVING (LOWER(SUBSTR(ms_DPProfileD.ScheduleID,1,7))='weekday' Or LOWER(SUBSTR(ms_DPProfileD.ScheduleID,1,7))='weekend') AND ms_DPPatternD.Sqn <> 0 \"\n",
    "        sql += \"ORDER BY scheduleid, time\"\n",
    "        diurnal_wws = sql_to_df(sql,model_path) \n",
    "        \n",
    "        sql = \"SELECT SUM(loadflow) FROM msm_Loadpoint WHERE description = 'Baseflow' and Active = 1\"\n",
    "        gwi = sql_to_df(sql,model_path).iloc[0,0]\n",
    "        \n",
    "        diurnal_wws.Hour = diurnal_wws.Hour - 1\n",
    "        diurnal_wws.DWF = diurnal_wws.DWF + gwi\n",
    "        \n",
    "        sql = \"SELECT COUNT(muid) FROM msm_Loadpoint WHERE description = 'BSF' and Active = 1\"\n",
    "        bsf_count = sql_to_df(sql,model_path).iloc[0,0]\n",
    "        bsf = 0\n",
    "        bsf_on = True if bsf_count > 0 else False\n",
    "        if bsf_on:\n",
    "            print('Import BSF')\n",
    "            sql = \"SELECT SUM(loadflow) FROM msm_Loadpoint WHERE description = 'BSF' and Active = 1\"\n",
    "            bsf = sql_to_df(sql,model_path).iloc[0,0]\n",
    "            sql = \"SELECT SUM(area) FROM msm_Catchment WHERE Active = 1\"\n",
    "            area = sql_to_df(sql,model_path).iloc[0,0]\n",
    "            ini_rate = round(bsf * 86400 / area * 10000 * 1000, 0)\n",
    "            ini_no = ini_rate / 11200 \n",
    "            \n",
    "        #Extract results\n",
    "        print('Import WWTP')\n",
    "        res1d = Res1D(result_network_path)\n",
    "        sim_start = res1d.time_index.min()\n",
    "        start = sim_start + timedelta(days=1)\n",
    "        end = res1d.time_index.max()\n",
    "        sim_seconds = (end - sim_start).total_seconds()\n",
    "        timesteps = len(res1d.time_index)-1\n",
    "        timestep_seconds = sim_seconds / timesteps\n",
    "        skip_steps = int(86400 / timestep_seconds)\n",
    "        \n",
    "        wwtp_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "        wwtp_df['WWTP'] = list(res1d.query.GetReachEndValues(wwtp_pipe, \"Discharge\"))[skip_steps:]\n",
    "        \n",
    "        print('Import outfalls')\n",
    "        outfall_df = pd.read_csv(outfall_summary)\n",
    "        first_round = True\n",
    "        for index, row in outfall_df.iterrows():\n",
    "            muid = row['Structure']\n",
    "            layer = row['Layer']\n",
    "            outfall = row['Outfall']\n",
    "            resid = muid\n",
    "            if layer.lower() != 'msm_link':\n",
    "                resid = layer[4:] + ':' + muid\n",
    "\n",
    "            overflow_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "            ts = list(res1d.query.GetReachEndValues(resid, \"Discharge\"))[skip_steps:]\n",
    "            overflow_df['MUID'] = muid\n",
    "            overflow_df['Overflow'] = ts\n",
    "            if first_round == True:\n",
    "                overflow_df_all = overflow_df.copy()\n",
    "            else:\n",
    "                overflow_df_all = pd.concat([overflow_df_all,overflow_df])                                                        \n",
    "            first_round = False\n",
    "    \n",
    "        if first_round == False:                        \n",
    "            overflow_df_all = overflow_df_all.groupby(overflow_df_all.index).agg({'Overflow': 'sum'})        \n",
    "        \n",
    "        print('Import inflow')\n",
    "        has_spill = False\n",
    "        first_round = True\n",
    "        for node in res1d.data.Nodes:\n",
    "            muid = node.Id\n",
    "            for i, flood_type in enumerate(flood_types):\n",
    "                ts = res1d.query.GetNodeValues(muid,flood_type)\n",
    "                if ts != None:\n",
    "                    if max(ts) > 0:\n",
    "                        spill_df = pd.DataFrame(index=res1d.time_index)\n",
    "                        spill_df['Node'] = muid\n",
    "                        spill_df['Spill'] = ts\n",
    "                        if first_round == True:\n",
    "                            spill_df_all = spill_df.copy()\n",
    "                        else:\n",
    "                            spill_df_all = pd.concat([spill_df_all,spill_df])                                                        \n",
    "                        first_round = False\n",
    "                        has_spill = True\n",
    "        \n",
    "        if first_round == False:                        \n",
    "            spill_df_all = spill_df_all.groupby(spill_df_all.index).agg({'Spill': 'sum'})\n",
    "            \n",
    "        print('Import runoff')\n",
    "        has_runoff = False\n",
    "        first_round = True\n",
    "        if os.path.exists(result_runoff_path):\n",
    "            res1d = Res1D(result_runoff_path)\n",
    "            has_runoff = True\n",
    "            for i, catchment in enumerate(res1d.data.Catchments):\n",
    "                ts_id = catchment.Id\n",
    "                if not ' - RDI' in ts_id and not ' - Kinematic wave (B)' in ts_id:\n",
    "#                     print('Importing catchment ' + str((i+1)/3) + ' of ' + str(len(res1d.data.Catchments)/3) + ': ' + muid)\n",
    "                    muid = ts_id\n",
    "                    ts = res1d.query.GetCatchmentValues(muid,'TotalRunOff')\n",
    "                    runoff_df = pd.DataFrame(index=res1d.time_index)\n",
    "                    runoff_df['Node'] = muid\n",
    "                    runoff_df['Runoff'] = ts\n",
    "                    if first_round == True:\n",
    "                        runoff_df_all = runoff_df.copy()\n",
    "                    else:\n",
    "                        runoff_df_all = pd.concat([runoff_df_all,runoff_df])                                                        \n",
    "                    first_round = False\n",
    "                    \n",
    "                           \n",
    "            runoff_df_all = runoff_df_all.groupby(runoff_df_all.index).agg({'Runoff': 'sum'})\n",
    "            \n",
    "        has_inflow = False\n",
    "        print('Import inflow')\n",
    "        sql = \"SELECT tsconnection, timeseriesname FROM msm_BBoundary WHERE active = 1 AND typeno = 9 \"\n",
    "        sql += \"AND variationno = 3\"\n",
    "        df = sql_to_df(sql,model_path)\n",
    "        first_round = True\n",
    "        for index, row in df.iterrows():\n",
    "            has_inflow = True\n",
    "            rel_path = row['tsconnection']\n",
    "            timeseriesname = row['timeseriesname']\n",
    "            dfs0_path = os.path.abspath(os.path.join(result_folder, rel_path))\n",
    "            res = mikeio.read(dfs0_path)\n",
    "            inflow_df = res.to_dataframe()\n",
    "            for i, col in enumerate(inflow_df.columns):\n",
    "                if col == timeseriesname:\n",
    "                    col_no = i\n",
    "            inflow_df = inflow_df[[timeseriesname]]\n",
    "            inflow_df['Boundary'] = timeseriesname\n",
    "            inflow_df.rename(columns={timeseriesname:'Inflow'},inplace=True)\n",
    "            inflow_df = inflow_df[['Boundary','Inflow']]\n",
    "            \n",
    "            if '(liter per sec)' in str(res.items[col_no]):\n",
    "                inflow_df.Inflow = inflow_df.Inflow/1000\n",
    "                \n",
    "            if first_round == True:\n",
    "                inflow_df_all = inflow_df.copy()\n",
    "            else:\n",
    "                inflow_df_all = pd.concat([inflow_df_all,inflow_df])                                                        \n",
    "            first_round = False\n",
    "        \n",
    "        if first_round == False:    \n",
    "            inflow_df_all = inflow_df_all.groupby(inflow_df_all.index).agg({'Inflow': 'sum'})   \n",
    "            \n",
    "        print('Done')\n",
    "                \n",
    "                \n",
    "\n",
    "        \n",
    "        \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5aa498",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsf = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8573db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = wwtp_df.copy()\n",
    "df_all = pd.merge(df_all, overflow_df_all, left_index=True, right_index=True, how='left')\n",
    "df_all = pd.merge(df_all, spill_df_all, left_index=True, right_index=True, how='left')\n",
    "df_all = pd.merge(df_all, inflow_df_all, left_index=True, right_index=True, how='left')\n",
    "df_all['Total Outflow'] = df_all.WWTP + df_all.Spill + df_all.Overflow\n",
    "df_all = pd.merge(df_all, runoff_df_all, left_index=True, right_index=True, how='left')\n",
    "df_all['DateTime'] = df_all.index\n",
    "df_all['Hour'] = df_all.DateTime.dt.hour\n",
    "df_all['Weekday'] = df_all['DateTime'].dt.day_name()\n",
    "df_all['Day_Type'] = 'Weekdays'\n",
    "df_all.loc[df_all['Weekday']=='Saturday','Day_Type']='Weekends'\n",
    "df_all.loc[df_all['Weekday']=='Sunday','Day_Type']='Weekends'\n",
    "df_all = pd.merge(df_all,diurnal_wws[['Day_Type', 'Hour','DWF']],on=['Day_Type', 'Hour'],how='inner')\n",
    "df_all.set_index('DateTime',inplace=True)\n",
    "df_all.sort_index(inplace=True)\n",
    "df_all['DWF'] = df_all['DWF'].rolling('1h').mean()\n",
    "df_all.fillna(method='bfill',inplace=True)\n",
    "df_all.drop(columns=['Hour','Weekday','Day_Type'],inplace=True)\n",
    "df_all['BSF'] = bsf\n",
    "df_all['Total Inflow'] = df_all.Inflow + df_all.Runoff + df_all.DWF + df_all.BSF\n",
    "df_all\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "for col in df_all.columns:   \n",
    "    \n",
    "\n",
    "    fig.add_trace(go.Scatter(x=df_all.index, \n",
    "                                     y = df_all[col], \n",
    "                                     mode='lines',name=col))\n",
    "\n",
    "    fig.update_layout(title = 'Model inflows and outflows')\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a765d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[['Total Inflow','Total Outflow']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d41818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#         sql = \"SELECT tsconnection, timeseriesname FROM msm_BBoundary WHERE active = 1 AND typeno = 9 \"\n",
    "#         sql += \"AND variationno = 3\"\n",
    "#         df = sql_to_df(sql,model_path)\n",
    "#         for index, row in df.iterrows():\n",
    "#             rel_path = row['tsconnection']\n",
    "#             timeseriesname = row['timeseriesname']\n",
    "#             dfs0_path = os.path.abspath(os.path.join(result_folder, rel_path))\n",
    "#             res = mikeio.read(dfs0_path)\n",
    "#             inflow = res.to_dataframe()\n",
    "#             for i, col in enumerate(inflow.columns):\n",
    "#                 if col == timeseriesname:\n",
    "#                     col_no = i\n",
    "#             inflow = inflow[[timeseriesname]]\n",
    "#             inflow['Boundary'] = timeseriesname\n",
    "#             inflow.rename(columns={timeseriesname:'Inflow'},inplace=True)\n",
    "#             inflow = inflow[['Boundary','Inflow']]\n",
    "            \n",
    "#             if '(liter per sec)' in str(res.items[col_no]):\n",
    "#                 inflow.Inflow = inflow.Inflow/1000\n",
    "#         inflow\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2727660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # res1d = Res1D(result_network_path)\n",
    "# skip_steps = 288\n",
    "# outfall_df = pd.read_csv(outfall_summary)\n",
    "# first_round = True\n",
    "# for index, row in outfall_df.iterrows():\n",
    "#     muid = row['Structure']\n",
    "#     layer = row['Layer']\n",
    "#     outfall = row['Outfall']\n",
    "#     resid = muid\n",
    "#     if layer.lower() != 'msm_link':\n",
    "#         resid = layer[4:] + ':' + muid\n",
    "#     print(resid)\n",
    "    \n",
    "#     overflow_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "#     ts = list(res1d.query.GetReachEndValues(resid, \"Discharge\"))[skip_steps:]\n",
    "#     overflow_df['MUID'] = muid\n",
    "#     overflow_df['Overflow'] = ts\n",
    "#     if first_round == True:\n",
    "#         overflow_df_all = overflow_df.copy()\n",
    "#     else:\n",
    "#         overflow_df_all = pd.concat([overflow_df_all,overflow_df])                                                        \n",
    "#     first_round = False\n",
    "    \n",
    "# if first_round == False:                        \n",
    "#     overflow_df_all = overflow_df_all.groupby(overflow_df_all.index).agg({'Overflow': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68292ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "overflow_df_all.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386925fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spill_df_all.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "runoff_df_all.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_mike",
   "language": "python",
   "name": "py39_mike"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
