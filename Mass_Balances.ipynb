{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bfa4f1",
   "metadata": {},
   "source": [
    "#Tool updated December 10 2024\n",
    "##Info\n",
    "<!-- \n",
    "\n",
    "To run this notebook, click menu Cell -> Run All\n",
    "\n",
    "Workflow:\n",
    "    1. Sum:\n",
    "        WW\n",
    "        GWI\n",
    "        I/I\n",
    "        Runoff\n",
    "        dfs0 inflow\n",
    "    2. Sum:\n",
    "        WWTP flow\n",
    "        MH Spilling\n",
    "        Outfalls\n",
    "        Delta volume\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb6340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 1\n",
    "\n",
    "import os\n",
    "import mikeio\n",
    "import mikeio1d\n",
    "from mikeio1d.res1d import Res1D\n",
    "from mikeio.dfs0 import Dfs0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ctypes\n",
    "import traceback\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA\n",
    "from Result_Lookup_Variables import *\n",
    "import subprocess\n",
    "import sqlite3\n",
    "import shutil\n",
    "from datetime import datetime as dt, timedelta\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198888cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql function\n",
    "def sql_to_df(sql,model):\n",
    "  con = sqlite3.connect(model)\n",
    "  df = pd.read_sql(sql, con)\n",
    "  con.close()\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5691a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing 1.5 I/I, 2030 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.4 I/I, 2035 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.35 I/I, 2040 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.3 I/I, 2045 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.25 I/I, 2050 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.15 I/I, 2060 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.0 I/I, 2075 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.0 I/I, 2090 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.0 I/I, 2100 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.5 I/I, 2030 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.4 I/I, 2035 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.35 I/I, 2040 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.3 I/I, 2045 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.25 I/I, 2050 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.15 I/I, 2060 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.0 I/I, 2075 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.0 I/I, 2090 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.0 I/I, 2100 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.5 I/I, 2030 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.4 I/I, 2035 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.35 I/I, 2040 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.3 I/I, 2045 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.25 I/I, 2050 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.15 I/I, 2060 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.0 I/I, 2075 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.0 I/I, 2090 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.0 I/I, 2100 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.5 I/I, 2030 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.4 I/I, 2035 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.35 I/I, 2040 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.3 I/I, 2045 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.25 I/I, 2050 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.15 I/I, 2060 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.0 I/I, 2075 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.0 I/I, 2090 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Importing 1.0 I/I, 2100 Pop\n",
      "Import DWF\n",
      "Import BSF\n",
      "Import WWTP\n",
      "Import Disconnections\n",
      "Import outfalls\n",
      "Import inflow\n",
      "Import spill\n",
      "Import runoff\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "flood_types = ['WaterFlowRateAboveGround','WaterSpillDischarge']\n",
    "cover_types = ['Normal','Spilling']\n",
    "# boundary_inflows = []\n",
    "# boundary_inflows.append('Landfill')\n",
    "\n",
    "for m in master_list:\n",
    "  \n",
    "    model_area = m[0]\n",
    "    model = m[1]\n",
    "    result_folder = m[2]\n",
    "    output_folder = m[3]\n",
    "    result_list = m[4]\n",
    "    groupby_acronym_owner = m[5]\n",
    "    element_filter = m[7]\n",
    "    wwtp_pipe = m[9]\n",
    "    outfall_summary = m[10]\n",
    "    \n",
    "    outfall_disconnection_df = pd.read_csv(outfall_summary,dtype={'Structure':'str'})\n",
    "    \n",
    "    if not os.path.isdir(output_folder + r'\\Maps_And_CSS'): os.makedirs(output_folder + r'\\Maps_And_CSS') \n",
    "    shutil.copy2('style_mb.css', output_folder + r'\\Maps_And_CSS\\style_mb.css')\n",
    "    shutil.copy2('script_mb.js', output_folder + r'\\Maps_And_CSS\\script_mb.js')\n",
    "    \n",
    "    #find subfolders for MIKE+\n",
    "    result_dict = {}\n",
    "    if model[-7:] == '.sqlite':       \n",
    "        not_founds = []\n",
    "        for r in result_list:\n",
    "            file = r[1]\n",
    "            file_found = False\n",
    "            for f1 in os.listdir(result_folder):\n",
    "                if f1[-7:] == '.sqlite':\n",
    "                    #browse subfolder\n",
    "                    result_subfolder = os.path.basename(f1)[:-7] + '_m1d - Result Files'\n",
    "                    try:\n",
    "                        for f2 in os.listdir(result_folder + '\\\\' + result_subfolder):\n",
    "                            if os.path.basename(f2) == file:\n",
    "                                result_dict[file] = [f1,'\\\\' + result_subfolder]\n",
    "                                file_found = True\n",
    "                    except:\n",
    "                        pass\n",
    "            if not file_found:\n",
    "                not_founds.append(file)\n",
    "                \n",
    "    else:\n",
    "\n",
    "        for r in result_list:\n",
    "            file = r[1]\n",
    "            run_model = r[4]\n",
    "            result_dict[file] = [run_model,'']\n",
    "        \n",
    " \n",
    "    output_subfolder = output_folder + '\\\\All_Mass_Balances'\n",
    "    if not os.path.isdir(output_subfolder): os.makedirs(output_subfolder) \n",
    "\n",
    "    html_path = output_subfolder + '\\\\Mass_Balance.html'\n",
    "    f = open(html_path, \"w\")\n",
    "    \n",
    "    f.write('<!DOCTYPE html>\\n')\n",
    "    f.write('<html>\\n')\n",
    "    f.write('<head>\\n')\n",
    "    f.write('<meta charset=\"utf-8\">\\n')\n",
    "    f.write('<link rel=\"stylesheet\" href=\"..\\Maps_And_CSS\\style_mb.css\">\\n')\n",
    "    f.write('<script src=\"..\\Maps_And_CSS\\script_mb.js\"></script>\\n')  \n",
    "    f.write('</head>\\n')\n",
    "    f.write('<body>\\n\\n')\n",
    "\n",
    "    f.write('<div class=\"tab\">\\n')\n",
    "    for r in result_list:\n",
    "        tab = r[0]       \n",
    "        f.write('  <button class=\"tablinks\" onclick=\"openTab(event, ' + \"'\" + tab + \"'\"  + ')\">' + tab + '</button>\\n')\n",
    "    f.write('</div>\\n') #f.write('<div class=\"tab\">\\n')\n",
    "       \n",
    "    \n",
    "    for r in result_list:\n",
    "        \n",
    "        link_dict = {}\n",
    "        \n",
    "        table_specs = []\n",
    "        table_specs.append(['Inflow and Outflow',['Total Inflow','Total Outflow']])\n",
    "        table_specs.append(['Inflow Breakdown',['Runoff','GWI','Wastewater','I/I','Boundary']])\n",
    "        table_specs.append(['Outflow Breakdown',['WWTP','Disconnection','Overflow','Spill']])\n",
    "        \n",
    "        header = r[0]\n",
    "        tab = header\n",
    "        file = r[1]\n",
    "        model_path = result_folder + '\\\\' + result_dict[file][0]\n",
    "        result_network_path = result_folder + '\\\\' + result_dict[file][1] + '\\\\' + file\n",
    "        if '.mdb' in model:\n",
    "            result_runoff_path = result_folder + '\\\\' + r[3] if len(r[3]) > 0 else 'No_Runoff'\n",
    "        else:\n",
    "            result_runoff_path = result_network_path[:-16] + 'Surface_runoff.res1d'\n",
    "        \n",
    "        individual_dfs = []\n",
    "        \n",
    "        print('Importing ' + header)\n",
    "        \n",
    "        f.write('<div id=\"' + tab + '\" class=\"tabcontent\">\\n')  \n",
    "        \n",
    "        \n",
    "#         f.write('<div class=\"row\"><div class=\"column\">\\n')\n",
    "        if model[-7:] == '.sqlite': \n",
    "            print('Import DWF')\n",
    "            sql = \"SELECT ms_DPProfileD.ScheduleID AS Day_Type, strftime('%H', time) + 1 AS [Hour], Sum(msm_Loadpoint.loadflow*ms_DPPatternD.DPValue) AS Wastewater \"\n",
    "            sql += \"FROM ((msm_Loadpoint INNER JOIN msm_BBoundary ON msm_Loadpoint.LoadCategoryNo = msm_BBoundary.LoadCategoryNo) INNER JOIN ms_DPProfileD ON msm_BBoundary.DPProfileID = ms_DPProfileD.ProfileID) INNER JOIN ms_DPPatternD ON ms_DPProfileD.PatternID = ms_DPPatternD.PatternID \"\n",
    "            sql += \"WHERE msm_Loadpoint.Active = 1 AND msm_Loadpoint.Enabled = 1 AND ms_DPProfileD.Active = 1 AND ms_DPPatternD.Active = 1 AND msm_BBoundary.Active = 1 \"\n",
    "            sql += \"GROUP BY ms_DPProfileD.ScheduleID, ms_DPPatternD.Time \"\n",
    "            sql += \"HAVING (LOWER(SUBSTR(ms_DPProfileD.ScheduleID,1,7))='weekday' Or LOWER(SUBSTR(ms_DPProfileD.ScheduleID,1,7))='weekend') AND ms_DPPatternD.Sqn <> 0 \"\n",
    "            sql += \"ORDER BY scheduleid, time\"\n",
    "            diurnal_wws = sql_to_df(sql,model_path)\n",
    "            diurnal_wws['Day_Type'] = diurnal_wws['Day_Type'].replace('weekday', 'Weekdays')\n",
    "            diurnal_wws['Day_Type'] = diurnal_wws['Day_Type'].replace('weekend', 'Weekends')\n",
    "            \n",
    "\n",
    "            sql = \"SELECT SUM(loadflow) FROM msm_Loadpoint WHERE loadcategory = 'Baseflow' and Active = 1 and enabled = 1\"\n",
    "            gwi = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "            diurnal_wws.Hour = diurnal_wws.Hour - 1\n",
    "    #         diurnal_wws.Wastewater = diurnal_wws.Wastewater\n",
    "\n",
    "            sql = \"SELECT COUNT(muid) FROM msm_Loadpoint WHERE loadcategory = 'BSF' and Active = 1 AND Enabled = 1\"\n",
    "            bsf_count = sql_to_df(sql,model_path).iloc[0,0]\n",
    "            bsf = 0\n",
    "            bsf_on = True if bsf_count > 0 else False\n",
    "            if bsf_on:\n",
    "                print('Import BSF')\n",
    "                sql = \"SELECT SUM(loadflow) FROM msm_Loadpoint WHERE loadcategory = 'BSF' and Active = 1 AND Enabled = 1\"\n",
    "                bsf = sql_to_df(sql,model_path).iloc[0,0]\n",
    "                sql = \"SELECT SUM(area) FROM msm_Catchment WHERE Active = 1\"\n",
    "                area = sql_to_df(sql,model_path).iloc[0,0]\n",
    "                ini_rate = round(bsf * 86400 / area * 10000 * 1000, 0)\n",
    "                ini_no = ini_rate / 11200 \n",
    "                \n",
    "        else:\n",
    "            parameter_script = r\"Read_DWF.py\"\n",
    "            bat_file_path = 'Read_DWF.bat'\n",
    "            bat_file = open(bat_file_path, \"w\")\n",
    "            bat_file.write(python_installation + ' \"' + parameter_script + '\" \"' + os.getcwd() + '\" \"' + model_path + '\"')\n",
    "            bat_file.close()\n",
    "            result = subprocess.call([bat_file_path]) \n",
    "            if os.path.exists(model_path) == False:\n",
    "                raise ValueError(\"The variable 'model' points to a path that does not exist: \" + model)\n",
    "            if result == 1: #Error\n",
    "                raise ValueError(\"The sub process threw an error. Please Locate the bat file: \" + bat_file_path + \", open it in notepad, \\\n",
    "                then add a new line and type in letters only: Pause. Double click the bat file to run it and it will show the error.\")\n",
    "\n",
    "            diurnal_wws = pd.read_csv('Diurnals.csv')\n",
    "            gwi_bsf = pd.read_csv('GWI_BSF.csv')\n",
    "            gwi_bsf.set_index('Category',inplace=True) \n",
    "            gwi = gwi_bsf.loc['GWI','Value']  \n",
    "            bsf = gwi_bsf.loc['BSF','Value'] \n",
    "            bsf_on = True if bsf > 0 else False\n",
    "            \n",
    "        #Extract results\n",
    "        print('Import WWTP')\n",
    "        res1d = Res1D(result_network_path)\n",
    "        sim_start = res1d.time_index.min()\n",
    "        start = sim_start + timedelta(days=1)\n",
    "        end = res1d.time_index.max()\n",
    "        sim_seconds = (end - sim_start).total_seconds()\n",
    "        timesteps = len(res1d.time_index)-1\n",
    "        timestep_seconds = sim_seconds / timesteps\n",
    "        skip_steps = int(86400 / timestep_seconds)\n",
    "#         timestep_seconds = timestep_seconds - skip_steps\n",
    "        reach_ids = list(reach.Id[:reach.Id.rfind('-')] for reach in res1d.data.Reaches)\n",
    "        \n",
    "        if skip_steps >= timesteps:\n",
    "            raise ValueError(file + ' is less than one day. It may have crashed or still running.')\n",
    "        \n",
    "        wwtp_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "        \n",
    "        wwtp_df['WWTP'] = list(res1d.query.GetReachEndValues(wwtp_pipe, \"Discharge\"))[skip_steps:]\n",
    "        \n",
    "        path = output_folder + '\\\\All_Elements\\\\' + model_area + '_Discharge_Link_' + wwtp_pipe + '.html'\n",
    "        link_dict['WWTP'] = '<td style=\"text-align: left\"><a href=\"' + path + '\" target=\"_blank\">WWTP</a></td>\\n'\n",
    "                \n",
    "        print('Import Disconnections')\n",
    "        disc_df = outfall_disconnection_df[outfall_disconnection_df.Type=='Disconnection']\n",
    "        first_round = True\n",
    "        has_disc = False\n",
    "        for index, row in disc_df.iterrows():\n",
    "            scenario = row['Scenario']\n",
    "            if scenario in file:\n",
    "                muid = row['Structure']\n",
    "                layer = row['Layer']\n",
    "                outfall = row['Outfall']\n",
    "                resid = muid\n",
    "                if layer.lower() != 'msm_link':\n",
    "                    resid = layer[4:] + ':' + muid\n",
    "\n",
    "                disc_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "                ts = list(res1d.query.GetReachEndValues(resid, \"Discharge\"))[skip_steps:]\n",
    "                disc_df['Outfall'] = outfall\n",
    "                disc_df['Disconnection'] = ts\n",
    "                if first_round == True:\n",
    "                    disc_df_all = disc_df.copy()\n",
    "                else:\n",
    "                    disc_df_all = pd.concat([disc_df_all,disc_df])                                                        \n",
    "                first_round = False\n",
    "                has_disc = True\n",
    "                \n",
    "        if has_disc:\n",
    "            disconnection_df = disc_df_all.pivot(columns='Outfall', values='Disconnection')\n",
    "            individual_dfs.append(['Disconnections',disconnection_df])\n",
    "            disc_df_all = disc_df_all.groupby(disc_df_all.index).agg({'Disconnection': 'sum'}) \n",
    "        \n",
    "        print('Import outfalls')\n",
    "        outfall_df = outfall_disconnection_df[outfall_disconnection_df.Type=='Overflow'][['Structure', 'Layer', 'Outfall']]\n",
    "        outfall_df.sort_values(by=['Outfall','Structure'],inplace=True)\n",
    "        outfall_df.reset_index(drop=True,inplace=True)\n",
    "        first_round = True\n",
    "        has_overflow = False\n",
    "        \n",
    "        overflow_by_structure_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "        overflow_links = {}\n",
    "        for index, row in outfall_df.iterrows():\n",
    "            \n",
    "            muid = row['Structure']\n",
    "            layer = row['Layer']\n",
    "            outfall = row['Outfall']\n",
    "            resid = muid\n",
    "            if layer.lower() != 'msm_link':\n",
    "                resid = layer[4:] + ':' + muid\n",
    "                \n",
    "            if resid in reach_ids:\n",
    "\n",
    "                overflow_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "                ts = list(res1d.query.GetReachEndValues(resid, \"Discharge\"))[skip_steps:]\n",
    "                overflow_df['MUID'] = muid\n",
    "                overflow_df['Outfall'] = outfall\n",
    "                overflow_df['Overflow'] = ts\n",
    "\n",
    "                item_name = muid + ' (to ' + outfall + ')'\n",
    "\n",
    "                overflow_by_structure_df[item_name] = ts\n",
    "\n",
    "                path = output_folder + '\\\\All_Elements\\\\' + model_area + '_Discharge_' + layer[4:] + '_' + muid + '.html'\n",
    "                link_dict[item_name] = '<td style=\"text-align: left\"><a href=\"' + path + '\" target=\"_blank\">' + item_name + '</a></td>\\n'\n",
    "\n",
    "                if first_round == True:\n",
    "                    overflow_df_all = overflow_df.copy()\n",
    "                else:\n",
    "                    overflow_df_all = pd.concat([overflow_df_all,overflow_df])                                                        \n",
    "                first_round = False\n",
    "                has_overflow = True\n",
    "                \n",
    "        if has_overflow:       \n",
    "            overflow_by_outfall_df = overflow_df_all.groupby([overflow_df_all.index,overflow_df_all.Outfall]).agg({'Overflow': 'sum'}) \n",
    "            overflow_by_outfall_df.reset_index(level='Outfall', inplace=True)\n",
    "            overflow_by_outfall_df = overflow_by_outfall_df.pivot(columns='Outfall', values='Overflow')\n",
    "            overflow_df_all = overflow_df_all.groupby(overflow_df_all.index).agg({'Overflow': 'sum'})\n",
    "            individual_dfs.append(['Outfalls',overflow_by_outfall_df])\n",
    "            individual_dfs.append(['Overflow Structures',overflow_by_structure_df])\n",
    "\n",
    "        has_boundary = False\n",
    "        if model[-7:] == '.sqlite':   \n",
    "            print('Import inflow')\n",
    "            sql = \"SELECT muid, tsconnection, variationno, constantvalue, timeseriesname FROM msm_BBoundary WHERE active = 1 AND typeno = 9 \"\n",
    "            sql += \"AND applyboundaryno = 1\"\n",
    "            df = sql_to_df(sql,model_path)\n",
    "            first_round = True\n",
    "            for index, row in df.iterrows():\n",
    "                has_boundary= True\n",
    "                if row['variationno'] == 1:\n",
    "                    inflow_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "                    inflow_df['Boundary'] = row['muid']\n",
    "                    inflow_df['Inflow'] = row['constantvalue']\n",
    "                else:\n",
    "                    rel_path = row['tsconnection']\n",
    "                    timeseriesname = row['timeseriesname']\n",
    "                    dfs0_path = os.path.abspath(os.path.join(result_folder, rel_path))\n",
    "                    res = mikeio.read(dfs0_path)\n",
    "                    inflow_df = res.to_dataframe()\n",
    "                    for i, col in enumerate(inflow_df.columns):\n",
    "                        if col == timeseriesname:\n",
    "                            col_no = i\n",
    "                    inflow_df = inflow_df[[timeseriesname]]\n",
    "                    inflow_df['Boundary'] = timeseriesname\n",
    "                    inflow_df.rename(columns={timeseriesname:'Inflow'},inplace=True)\n",
    "                    inflow_df = inflow_df[['Boundary','Inflow']]\n",
    "                    ref_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "                    inflow_df = pd.merge(ref_df,inflow_df,left_index=True, right_index=True,how='left').fillna(method='bfill')\n",
    "\n",
    "                    if '(liter per sec)' in str(res.items[col_no]):\n",
    "                        inflow_df.Inflow = inflow_df.Inflow/1000\n",
    "\n",
    "                if first_round == True:\n",
    "                    inflow_df_all = inflow_df.copy()\n",
    "                else:\n",
    "                    inflow_df_all = pd.concat([inflow_df_all,inflow_df])                                                        \n",
    "                first_round = False\n",
    "        \n",
    "        if has_boundary:\n",
    "            inflow_by_boundary_df = inflow_df_all.pivot(columns='Boundary', values='Inflow')\n",
    "            inflow_by_boundary_df = inflow_by_boundary_df.loc[start:end]\n",
    "            inflow_by_boundary_df.fillna(method='bfill',inplace=True)\n",
    "            #It is unknown why a nan column is sometimes created but the below will remove it.\n",
    "            if np.nan in list(inflow_by_boundary_df.columns):\n",
    "                inflow_by_boundary_df.drop(columns=[np.nan],inplace=True)\n",
    "            individual_dfs.append(['Boundaries',inflow_by_boundary_df])\n",
    "            inflow_df_all = inflow_df_all.groupby(inflow_df_all.index).agg({'Inflow': 'sum'}) \n",
    "            inflow_df_all.rename(columns={'Inflow':'Boundary'},inplace=True) \n",
    "            \n",
    "            \n",
    "        print('Import spill')\n",
    "        if '.mdb' in model:\n",
    "            result_network_path = result_network_path[:-6] + '.ADDOUT.res1d'\n",
    "            \n",
    "        has_spill = False\n",
    "        first_round = True\n",
    "        if os.path.exists(result_network_path):\n",
    "            res1d = Res1D(result_network_path)\n",
    "            for node in res1d.data.Nodes:\n",
    "                muid = node.Id\n",
    "                for i, flood_type in enumerate(flood_types):\n",
    "                    ts = res1d.query.GetNodeValues(muid,flood_type)\n",
    "                    if ts != None:\n",
    "                        if max(ts) > 0:\n",
    "\n",
    "                            path = output_folder + '\\\\All_Elements\\\\' + model_area + '_WaterLevel_Node_' + muid + '.html'\n",
    "                            link_dict[muid] = '<td style=\"text-align: left\"><a href=\"' + path + '\" target=\"_blank\">' + muid + '</a></td>\\n'\n",
    "\n",
    "                            spill_df = pd.DataFrame(index=res1d.time_index)[skip_steps:]\n",
    "                            spill_df['Node'] = muid\n",
    "                            spill_df['Spill'] = list(ts)[skip_steps:]\n",
    "                            if first_round == True:\n",
    "                                spill_df_all = spill_df.copy()\n",
    "                            else:\n",
    "                                spill_df_all = pd.concat([spill_df_all,spill_df])                                                        \n",
    "                            first_round = False\n",
    "                            has_spill = True\n",
    "        \n",
    "        if has_spill:\n",
    "            spill_by_node_df = spill_df_all.pivot(columns='Node', values='Spill')\n",
    "            ordered_cols = [muid for muid in spill_by_node_df.sum().sort_values(ascending=False).index]\n",
    "            spill_by_node_df = spill_by_node_df[ordered_cols]\n",
    "            individual_dfs.append(['Spills',spill_by_node_df])\n",
    "            spill_df_all = spill_df_all.groupby(spill_df_all.index).agg({'Spill': 'sum'})\n",
    "            \n",
    "        print('Import runoff')\n",
    "        has_runoff = False\n",
    "        first_round = True\n",
    "        if os.path.exists(result_runoff_path):\n",
    "            res1d = Res1D(result_runoff_path)\n",
    "            has_runoff = True\n",
    "            for i, catchment in enumerate(res1d.data.Catchments):\n",
    "                ts_id = catchment.Id\n",
    "                if not ' - RDI' in ts_id and not ' - Kinematic wave (B)' in ts_id:\n",
    "#                     print('Importing catchment ' + str((i+1)/3) + ' of ' + str(len(res1d.data.Catchments)/3) + ': ' + muid)\n",
    "                    muid = ts_id\n",
    "                    ts = res1d.query.GetCatchmentValues(muid,'TotalRunOff')\n",
    "                    runoff_df = pd.DataFrame(index=res1d.time_index)\n",
    "                    runoff_df['Node'] = muid\n",
    "                    runoff_df['Runoff'] = ts\n",
    "                    if first_round == True:\n",
    "                        runoff_df_all = runoff_df.copy()\n",
    "                    else:\n",
    "                        runoff_df_all = pd.concat([runoff_df_all,runoff_df])                                                        \n",
    "                    first_round = False\n",
    "                    \n",
    "                           \n",
    "            runoff_df_all = runoff_df_all.groupby(runoff_df_all.index).agg({'Runoff': 'sum'})\n",
    "                                \n",
    "        df_all = wwtp_df.copy()\n",
    "        if has_overflow:\n",
    "            df_all = pd.merge(df_all, overflow_df_all, left_index=True, right_index=True, how='left')\n",
    "        else:\n",
    "            df_all['Overflow'] = 0\n",
    "        if has_disc:\n",
    "            df_all = pd.merge(df_all, disc_df_all, left_index=True, right_index=True, how='left')\n",
    "        else:\n",
    "            df_all['Disconnection'] = 0\n",
    "        if has_spill:\n",
    "            df_all = pd.merge(df_all, spill_df_all, left_index=True, right_index=True, how='left')\n",
    "        else:\n",
    "            df_all['Spill'] = 0\n",
    "        if has_boundary:\n",
    "            df_all = pd.merge(df_all, inflow_df_all, left_index=True, right_index=True, how='left')\n",
    "        else:\n",
    "            df_all['Boundary'] = 0\n",
    "        df_all['Total Outflow'] = df_all.WWTP + df_all.Spill + df_all.Overflow + df_all.Disconnection\n",
    "        if has_runoff:\n",
    "            df_all = pd.merge(df_all, runoff_df_all, left_index=True, right_index=True, how='left')\n",
    "        else:\n",
    "            df_all['Runoff'] = 0\n",
    "             \n",
    "            \n",
    "        df_all['DateTime'] = df_all.index\n",
    "        df_all['Hour'] = df_all.DateTime.dt.hour\n",
    "        df_all['Weekday'] = df_all['DateTime'].dt.day_name()\n",
    "        df_all['Day_Type'] = 'Weekdays'\n",
    "        df_all.loc[df_all['Weekday']=='Saturday','Day_Type']='Weekends'\n",
    "        df_all.loc[df_all['Weekday']=='Sunday','Day_Type']='Weekends'\n",
    "        df_all = pd.merge(df_all,diurnal_wws[['Day_Type', 'Hour','Wastewater']],on=['Day_Type', 'Hour'],how='inner')\n",
    "        df_all.set_index('DateTime',inplace=True)\n",
    "        df_all.sort_index(inplace=True)\n",
    "        df_all['Wastewater'] = df_all['Wastewater'].rolling('1h').mean()\n",
    "        df_all.fillna(method='bfill',inplace=True)\n",
    "        df_all.drop(columns=['Hour','Weekday','Day_Type'],inplace=True)\n",
    "        df_all['GWI'] = gwi\n",
    "        df_all['I/I'] = bsf\n",
    "        df_all['Total Inflow'] = df_all.Boundary + df_all.Runoff + df_all.GWI + df_all.Wastewater + df_all['I/I']\n",
    "                \n",
    "        maxes = df_all.max()\n",
    "        sums = df_all.sum()\n",
    "        \n",
    "        for individual_df in individual_dfs:  \n",
    "            table_specs.append([individual_df[0],list(individual_df[1].columns)])\n",
    "            maxes = pd.concat([maxes,individual_df[1].max()])\n",
    "            sums = pd.concat([sums,individual_df[1].sum()])\n",
    "        \n",
    "        f.write('<div class=\"sidenav\">\\n')\n",
    "        for table_spec in table_specs:\n",
    "            items = table_spec[1]\n",
    "            f.write('<h2>' + table_spec[0] + '</h2>\\n')\n",
    "            f.write('<table>\\n')\n",
    "            f.write('<tr>\\n')\n",
    "            f.write('<th rowspan=\"2\">Description</th>\\n') \n",
    "            f.write('<th style=\"text-align: center\">Volume</th>\\n') \n",
    "            f.write('<th style=\"text-align: center\">Peak Flow</th>\\n') \n",
    "            f.write('</tr>\\n')\n",
    "            f.write('<tr>\\n')\n",
    "            f.write('<th style=\"text-align: center\">ML</th>\\n')\n",
    "            f.write('<th style=\"text-align: center\">L/s</th>\\n') \n",
    "            f.write('</tr>\\n')\n",
    "            \n",
    "            for item in items:\n",
    "                \n",
    "                f.write('<tr>\\n')\n",
    "#                 if table_spec[0] == 'Spills':\n",
    "#                     path = output_folder + '\\\\All_Elements\\\\' + model_area + '_WaterLevel_Node_' + item + '.html'\n",
    "#                     f.write('<td style=\"text-align: left\"><a href=\"' + path + '\" target=\"_blank\">'+ item + '</a></td>\\n')\n",
    "#                 elif item == 'WWTP':\n",
    "#                     path = output_folder + '\\\\All_Elements\\\\' + model_area + '_Discharge_Link_' + wwtp_pipe + '.html'\n",
    "#                     f.write('<td style=\"text-align: left\"><a href=\"' + path + '\" target=\"_blank\">'+ item + '</a></td>\\n')\n",
    "#                 else:\n",
    "                try:\n",
    "                    f.write(link_dict[item])\n",
    "                except:\n",
    "                    f.write('<td style=\"text-align: left\">' + item + '</td>\\n')\n",
    "                volume = int(sums[item]*timestep_seconds/1000) if not np.isnan(sums[item]) else 0\n",
    "                f.write('<td style=\"text-align: right\">'+ str(volume) + '</td>\\n')\n",
    "                flow = int(maxes[item]*1000) if not np.isnan(maxes[item]) else 0\n",
    "                f.write('<td style=\"text-align: right\">'+ str(flow) + '</td>\\n')\n",
    "\n",
    "                f.write('</tr>\\n')\n",
    "            f.write('</table>\\n')\n",
    "        f.write('<h1 style=\"color: white\">End of tables</h1>\\n')#Invisible, just to enable scroll to table bottoms\n",
    "        f.write('<h1 style=\"color: white\">End of tables</h1>\\n')\n",
    "        f.write('<h1 style=\"color: white\">End of tables</h1>\\n')\n",
    "        f.write('<h1 style=\"color: white\">End of tables</h1>\\n')\n",
    "        f.write('<h1 style=\"color: white\">End of tables</h1>\\n')\n",
    "        f.write('</div>\\n')\n",
    "        f.write('<div class=\"main\">\\n')\n",
    "        f.write('<h1>' + tab + '</h1>')\n",
    "#         f.write('<div class=\"column\">\\n')\n",
    "        \n",
    "        grouped_cols = [list(individual_df[1].columns) for individual_df in individual_dfs]\n",
    "        cols = [col for sublist in grouped_cols for col in sublist]\n",
    "        \n",
    "        all_cols = list(df_all.columns) + cols       \n",
    "\n",
    "        buttons_ons = []\n",
    "        buttons_ons.append(['Inflow and Outflow',['Total Inflow','Total Outflow']])\n",
    "        buttons_ons.append(['Inflows',['Runoff','GWI','Wastewater','Boundary','I/I','Total Inflow']])\n",
    "        buttons_ons.append(['Outflows',['WWTP','Disconnection','Overflow','Spill','Total Outflow']])\n",
    "        \n",
    "        for individual_df in individual_dfs:\n",
    "            stophere = 9 if individual_df[0] == 'Spills' else len(individual_df[1].columns)\n",
    "            button_name = individual_df[0] + ' Top 10' if individual_df[0] == 'Spills' else individual_df[0]\n",
    "            buttons_ons.append([button_name,list(individual_df[1].columns[:stophere])])\n",
    "\n",
    "#         falses = [False for i in range(sum([len(individual_df[1].columns) for individual_df in individual_dfs]))]\n",
    "#         individuals = len(falses)\n",
    "        for button_ons in buttons_ons:\n",
    "            on_list = []\n",
    "            for col in all_cols:\n",
    "                on_list.append(col in button_ons[1])\n",
    "            button_ons[1] = on_list\n",
    "#             button_ons[1] += falses\n",
    "        buttons_ons\n",
    "\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        for col in df_all.columns:   \n",
    "\n",
    "            is_visible = True if col in ['Total Inflow','Total Outflow'] else False\n",
    "            fig.add_trace(go.Scatter(x=df_all.index, \n",
    "                                             y = df_all[col], \n",
    "                                             mode='lines',name=col, visible=is_visible))\n",
    "        for individual_df in individual_dfs:\n",
    "            stophere = 9 if individual_df[0] == 'Spills' else len(individual_df[1].columns)\n",
    "            \n",
    "            for col in individual_df[1].columns[:stophere]:\n",
    "                fig.add_trace(go.Scatter(x=individual_df[1].index, \n",
    "                                             y = individual_df[1][col], \n",
    "                                             mode='lines',name=col, visible=False, showlegend=True ))\n",
    "\n",
    "\n",
    "        fig.update_layout(\n",
    "            title = 'Model inflows and outflows',\n",
    "            autosize=False,\n",
    "            width = 1362,\n",
    "            height=800,\n",
    "            margin=dict(\n",
    "                l=50,\n",
    "                r=50,\n",
    "                b=25,\n",
    "                t=35,\n",
    "                pad=4\n",
    "                ),\n",
    "            yaxis_title='Discharge (cms)',                        \n",
    "\n",
    "            updatemenus=[\n",
    "                {\n",
    "                    'buttons': [\n",
    "                        {\n",
    "                            'args': [{'visible': button_on[1]}, {'title': button_on[0]}],\n",
    "                            'label': button_on[0],\n",
    "                            'method': 'update'\n",
    "                        }\n",
    "                        for button_on in buttons_ons\n",
    "                    ],\n",
    "                    'direction': 'left',\n",
    "                    'pad': {'r': 10, 't': 0},\n",
    "#                     'pad': {'r': 10, 't': 87},\n",
    "                    'showactive': True,\n",
    "                    'type': 'buttons',\n",
    "                    'x': 0.1,\n",
    "                    'xanchor': 'left',\n",
    "                    'y': 1,\n",
    "#                     'y': 0.06,\n",
    "                    'yanchor': 'top'\n",
    "                }\n",
    "                ]                                           \n",
    "            )\n",
    "#         fig['layout']['yaxis']['range']=[0:]\n",
    "\n",
    "        f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "        f.write('</div>\\n') #f.write('<div class=\"main\">\\n')\n",
    "        f.write('</div>\\n') #f.write('<div id=\"' + tab + '\" class=\"tabcontent\">\\n') \n",
    "#         f.write('</div>\\n')\n",
    "\n",
    "        df_all['Header'] = header\n",
    "        overflow_by_structure_df['Header'] = header\n",
    "        \n",
    "        df_all = df_all[list(df_all.columns)[-1:]+list(df_all.columns)[:-1]]\n",
    "        overflow_by_structure_df = overflow_by_structure_df[list(overflow_by_structure_df.columns)[-1:]+list(overflow_by_structure_df.columns)[:-1]]\n",
    "        \n",
    "        if not 'df_all_export' in locals():\n",
    "            df_all_export = df_all.copy()\n",
    "            overflow_by_structure_df_export = overflow_by_structure_df.copy()\n",
    "        else:\n",
    "            df_all_export = pd.concat([df_all_export,df_all])\n",
    "            overflow_by_structure_df_export = pd.concat([overflow_by_structure_df_export,overflow_by_structure_df])\n",
    "            \n",
    "    df_all_export = df_all_export[['Header', 'WWTP', 'Overflow', 'Disconnection', 'Spill', 'Boundary',\n",
    "       'Total Outflow', 'Runoff', 'Wastewater', 'GWI', 'I/I', 'Total Inflow']]\n",
    "    df_all_export.to_excel(output_subfolder + '\\\\Mass_Balance_Totals.xlsx')\n",
    "    overflow_by_structure_df_export.to_excel(output_subfolder + '\\\\Mass_Balance_Structures.xlsx')\n",
    "    \n",
    "    \n",
    "    f.write('</body>\\n')\n",
    "    f.write('</html>\\n')\n",
    "    f.close()\n",
    "        \n",
    "        \n",
    "print('Done')                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_mike",
   "language": "python",
   "name": "py39_mike"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
