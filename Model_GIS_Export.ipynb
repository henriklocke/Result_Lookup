{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bfa4f1",
   "metadata": {},
   "source": [
    "#Tool updated October 28 2024\n",
    "##Info\n",
    "<!-- \n",
    "To run this notebook, click menu Cell -> Run All\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb6340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 1\n",
    "\n",
    "import os\n",
    "import mikeio\n",
    "import mikeio1d\n",
    "from mikeio1d.res1d import Res1D\n",
    "from mikeio.dfs0 import Dfs0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlwings as xw\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ctypes\n",
    "import traceback\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA\n",
    "from Model_GIS_Export_Variables import *\n",
    "import subprocess\n",
    "import sqlite3\n",
    "import shutil\n",
    "from datetime import datetime as dt, timedelta\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3779f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid file name WWTP.xlsx\n",
      "Warning: No FacilityID found for MH20 in GNK\n",
      "Warning: No FacilityID found for MH21 in GNK\n",
      "Warning: No FacilityID found for MH22 in GNK\n",
      "Warning: No FacilityID found for MH24 in GNK\n",
      "Warning: No FacilityID found for MH25 in GNK\n",
      "Warning: No FacilityID found for MH26 in GNK\n",
      "Open/saved/closed or skipped 100 sheets.\n",
      "Warning: No FacilityID found for MH8 in HLR\n",
      "Warning: No FacilityID found for MH27 in HLY\n",
      "Open/saved/closed or skipped 200 sheets.\n",
      "Warning: No FacilityID found for MH in NVC\n",
      "Warning: No FacilityID found for MH29 in NVD\n",
      "Warning: No FacilityID found for MH5B in NVD\n",
      "Open/saved/closed or skipped 300 sheets.\n",
      "Warning: No FacilityID found for MH30 in NVY2\n",
      "Warning: No FacilityID found for MH31 in NVY2\n",
      "Open/saved/closed or skipped 400 sheets.\n",
      "Warning: No FacilityID found for MH30 in NVY\n",
      "Warning: No FacilityID found for MH31 in NVY\n",
      "Warning: No FacilityID found for MH6 in NVY\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 2\n",
    "#Import RAWN\n",
    "\n",
    "try:\n",
    "    \n",
    "    if open_save_close and rawn_input_from_model:\n",
    "        messageText = \"WARNING: open_save_close set to True which will open/save/close all RAWN sheets\\n\\n\"\n",
    "        messageText += \"This is only needed if RAWN sheets newly created by Tool 2 (formulas must be calculated).\\n\\n\"\n",
    "        messageText += \"You should close heavy spreadsheets before running. You screen will flicker.\\n\\n\"\n",
    "        messageText += \"Continue?\"\n",
    "        if MessageBox(None, messageText.encode('utf-8'), b'Info', 4) == 7:\n",
    "            MessageBox(None, b\"Execution cancelled.\", b'Info', 0)\n",
    "            raise Exception(b\"Execution cancelled by user.\")\n",
    "\n",
    "    columns = ['Key','SewerageArea', 'Acronym', 'MHName', 'FacilityID']    \n",
    "    \n",
    "    for rawn_year in rawn_years:\n",
    "        columns.append('PWWF_' + str(rawn_year))\n",
    "    rawn_output_df = pd.DataFrame(columns=columns)\n",
    "    rawn_output_df.set_index('Key',inplace=True)\n",
    "\n",
    "    if rawn_input_from_model:\n",
    "        #How many to rows to skip for this spreadsheet type\n",
    "        rawn_input_skiprows = 14\n",
    "        #The PWWF column name in this spreadsheet type\n",
    "        rawn_pwwf_column = 'PWWF (L/s)'\n",
    "    else:\n",
    "        rawn_input_skiprows = 13\n",
    "        rawn_pwwf_column = 'P.W.W.F.'\n",
    "\n",
    "    rawn_list = []\n",
    "\n",
    "    #This csv files contains matched facilityID to MUID, where a match was found in tool 1.\n",
    "    node_df = pd.read_csv(model_manhole_csv, dtype={'facilityid': str,'muid': str})\n",
    "\n",
    "    rawn_list = []\n",
    "    if not rawn_input_from_model: #Read csv file\n",
    "        rawn_input_df = pd.read_csv(rawn_csv)\n",
    "    else: #create dataframe similar to the rawn csv file by looping through the RAWN Excel file names\n",
    "        for rawn_inputfolder in rawn_inputfolders:\n",
    "            model_area = rawn_inputfolder[0]\n",
    "            folder = rawn_inputfolder[1]\n",
    "            model_versions = set()\n",
    "            mpf_versions = set()\n",
    "            for f in os.listdir(folder):\n",
    "\n",
    "                if f[-5:]=='.xlsx':\n",
    "                    filepath = folder + '\\\\' + f\n",
    "                    comb_name =  os.path.splitext(f)[0]\n",
    "                    if '_' in comb_name:\n",
    "                        acronym = comb_name.split('_')[0]\n",
    "                        mh_name = comb_name.split('_')[1]\n",
    "                        rawn_list.append([model_area,acronym,mh_name,comb_name,filepath])\n",
    "                        #Check for obsolete sheets \n",
    "                        df = pd.read_excel(filepath, sheet_name=comb_name)\n",
    "                        model_versions.add(df.iloc[2,9])\n",
    "                        mpf_versions.add(df.iloc[3,9] + ' - ' + df.iloc[4,9]) \n",
    "                    else:\n",
    "                        print(f'Invalid file name {f}')\n",
    "                        \n",
    "            if len(model_versions) > 1 or len(mpf_versions) > 1:\n",
    "                message = f'Tool ends!\\n\\nRAWN sheets from more than one model or mpf version found in folder:\\n\\n'\n",
    "                message += folder + '\\n\\nThis is not allowed. You can delete the sheets in the folder and replace them from the latest backup folder.\\n\\n'\n",
    "                message += 'Model versions:\\n'\n",
    "                for model_version in model_versions:\n",
    "                    message += str(model_version) + '\\n'\n",
    "                message += '\\nMPF versions:\\n'\n",
    "                for mpf_version in mpf_versions:\n",
    "                    message += str(mpf_version) + '\\n'\n",
    "                print(message)\n",
    "                MessageBox(None, message.encode('utf-8'), b'Error', 0)\n",
    "                raise Exception(message)\n",
    "                                           \n",
    "        rawn_input_df = pd.DataFrame(rawn_list,columns=['Sewer_Area', 'Acronym', 'MH_Name', 'Tab', 'Sheet_Path'])\n",
    "\n",
    "    for index1, row1 in rawn_input_df.iterrows():\n",
    "        if rawn_input_from_model:            \n",
    "            if open_save_close and rawn_input_from_model:\n",
    "                if len(open_save_close_filter) == 0 or row1['Sewer_Area'] in open_save_close_filter:\n",
    "                    app = xw.App(visible=False)                \n",
    "                    # Open, save, and close the workbook\n",
    "                    wb = app.books.open(row1['Sheet_Path'])\n",
    "                    wb.save()\n",
    "                    wb.close()\n",
    "                    app.quit()\n",
    "                    if (index1 + 1) % 100 == 0:\n",
    "                        print(f\"Open/saved/closed or skipped {index1 + 1} sheets.\")\n",
    "            rawn_single_df = pd.read_excel(row1['Sheet_Path'],sheet_name=row1['Tab'],skiprows=rawn_input_skiprows,\n",
    "                                           dtype={'NODE': str})\n",
    "            #This type has an empty row to be removed, may improve in later version\n",
    "            rawn_single_df.dropna(how='all', inplace=True)\n",
    "            #Reset the index to start at 0 \n",
    "            rawn_single_df.reset_index(inplace=True,drop=True)\n",
    "        else:\n",
    "            rawn_single_df = pd.read_excel(row1['Sheet_Path'],sheet_name=row1['Tab'],skiprows=rawn_input_skiprows)\n",
    "        sewer_area = row1['Sewer_Area']\n",
    "        acronym = row1['Acronym']\n",
    "        mh_name = row1['MH_Name']\n",
    "        index_val = acronym + '_' + mh_name\n",
    "        if rawn_input_from_model:\n",
    "            #The muid are in all rows, the below just takes the one from the first row. \n",
    "            muid = rawn_single_df.loc[0,'NODE']\n",
    "            node_match_df = node_df.loc[(node_df.muid==muid) & (node_df.sewer_area)]\n",
    "        else:\n",
    "            node_match_df = node_df.loc[(node_df.acronym==acronym) & (node_df.mhname==mh_name) & (node_df.sewer_area)]\n",
    "        node_match_df.reset_index(inplace=True)\n",
    "        if len(node_match_df) == 0:\n",
    "            print('Warning: No FacilityID found for ' + mh_name + ' in ' + acronym)\n",
    "            facilityid = 'Not Found'\n",
    "        else:\n",
    "            facilityid = node_match_df.loc[0,'facilityid']\n",
    "\n",
    "        rawn_output_df.loc[index_val,['SewerageArea','Acronym','MHName','FacilityID']]=[sewer_area,acronym,mh_name,facilityid]\n",
    "\n",
    "        #For traditional rawn sheets, the first 4 columns have headers one higher up than the following columns \n",
    "        #(which are under merged cells) so this adjustment below are only for traditional RAWN sheets.\n",
    "        if not rawn_input_from_model:\n",
    "            #In this version, the column names are in different rows, this is corrected below.\n",
    "            for i in range(4):\n",
    "                col_name = rawn_single_df.columns[i]\n",
    "                #Transfer the column name to the first row to be consistent with other columns\n",
    "                rawn_single_df.loc[0,col_name]=col_name\n",
    "            for col in rawn_single_df.columns:\n",
    "                #Set the column names to equal the value in the first row\n",
    "                rawn_single_df.rename(columns={col:rawn_single_df.loc[0,col]},inplace=True)\n",
    "            #Drop the first row which previously had the column names.\n",
    "            rawn_single_df.drop([0,1],inplace=True)\n",
    "        for index2, row2 in rawn_single_df.iterrows():\n",
    "            year = int(row2['YEAR'])\n",
    "            pwwf = row2[rawn_pwwf_column]\n",
    "            if year in rawn_years:\n",
    "                #Input in columns named after the year. This will create the column when it does not exist, otherwise just fill.\n",
    "                rawn_output_df.loc[index_val,'PWWF_' + str(year)] = pwwf\n",
    "\n",
    "    columns_to_check_for_nan = ['PWWF_' + str(rawn_year) for rawn_year in rawn_years]\n",
    "\n",
    "    if rawn_output_df[columns_to_check_for_nan].isna().all(axis=1).sum()>0:\n",
    "        messageText = \"WARNING: NAN values found in the output\\n\\n\"\n",
    "        messageText += \"The RAWN sheets likely need to be opened/closed/saved (formulas must be calculated).\\n\\n\"\n",
    "        messageText += \"You can stop the execution here (click 'No') and then set open_save_close=True to do this.\\n\\n\"\n",
    "        messageText += \"Continue?\"\n",
    "        if MessageBox(None, messageText.encode('utf-8'), 'Info', 4) == 7:\n",
    "            MessageBox(None, b\"Execution cancelled.\", b'Info', 0)\n",
    "            raise Exception(b\"Execution cancelled by user.\")\n",
    "\n",
    "    if len(acronym_filter) > 0:\n",
    "        #If only some acronyms are to be created\n",
    "        rawn_output_df = rawn_output_df[rawn_output_df.Acronym.isin(acronym_filter)]\n",
    "    \n",
    "    #Remove duplicates and not founds\n",
    "    rawn_output_df = rawn_output_df[rawn_output_df['FacilityID'] != 'Not Found']\n",
    "    rawn_duplicates = rawn_output_df[rawn_output_df.duplicated(subset=['FacilityID'], keep=False)]\n",
    "    rawn_duplicates = set(rawn_duplicates['FacilityID'])\n",
    "    rawn_duplicates = ','.join(map(str, rawn_duplicates))\n",
    "    rawn_output_df = rawn_output_df.drop_duplicates(subset=['FacilityID'], keep='first')\n",
    "\n",
    "    #Create the output csv\n",
    "    rawn_output_df.to_csv(output_folder + '\\\\RAWN_Nodes.csv', index=False) \n",
    "    \n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 2', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5691a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process VSA_DWF_No_Tide_2025pop_Base.res1d\n",
      "process VSA_WWF_EX-2yr-24hr-SCS1A_2025pop_Base.res1d\n",
      "process VSA_WWF_EX-5yr-24hr-SCS1A_2025pop_Base.res1d\n",
      "process VSA_WWF_EX-10yr-24hr-SCS1A_2025pop_Base.res1d\n",
      "process VSA_WWF_EX-25yr-24hr-SCS1A_2025pop_Base.res1d\n",
      "process VSA_DWF_No_Tide_2030pop_Base.res1d\n",
      "process VSA_WWF_EX-2yr-24hr-SCS1A_2030pop_Base.res1d\n",
      "process VSA_WWF_EX-5yr-24hr-SCS1A_2030pop_Base.res1d\n",
      "process VSA_WWF_EX-10yr-24hr-SCS1A_2030pop_Base.res1d\n",
      "process VSA_WWF_EX-25yr-24hr-SCS1A_2030pop_Base.res1d\n",
      "process FSA_DWF_2021-07-22_4d_2025pop_BaseDefault_Network_HD.res1d\n",
      "process FSA_GA_EX-2y-24h-AES_2025p_Base-DSS1Default_Network_HD.res1d\n",
      "process FSA_GA_EX-5y-24h-AES_2025p_Base-DSS2Default_Network_HD.res1d\n",
      "process FSA_GA_EX-10y-24h-AES_2025p_Base-DSS3Default_Network_HD.res1d\n",
      "process FSA_GA_EX-25y-24h-AES_2025p_Base-DSS16Default_Network_HD.res1d\n",
      "process FSA_DWF_2021-07-22_4d_2030pop_2030_NetworkDefault_Network_HD.res1d\n",
      "process FSA_GA_EX-2y-24h-AES_2030p_F_2030_Network-DSS4Default_Network_HD.res1d\n",
      "process FSA_GA_EX-5y-24h-AES_2030p_F_2030_Network-DSS5Default_Network_HD.res1d\n",
      "process FSA_GA_EX-10y-24h-AES_2030p_F_2030_Network-DSS6Default_Network_HD.res1d\n",
      "process FSA_GA_EX-25y-24h-AES_2030p_F_2030_Network-DSS17Default_Network_HD.res1d\n",
      "process NSSA_DWF_2018-07-26_4d_2025pop_BaseDefault_Network_HD.res1d\n",
      "process NSSA_GA_EX-2y-24h-AES_2025p_Base-DSS1Default_Network_HD.res1d\n",
      "process NSSA_GA_EX-5y-24h-AES_2025p_Base-DSS2Default_Network_HD.res1d\n",
      "process NSSA_GA_EX-10y-24h-AES_2025p_Base-DSS3Default_Network_HD.res1d\n",
      "process NSSA_GA_EX-25y-24h-AES_2025p_Base-DSS10Default_Network_HD.res1d\n",
      "process NSSA_DWF_2018-07-26_4d_2030pop_BaseDefault_Network_HD.res1d\n",
      "process NSSA_GA_EX-2y-24h-AES_2030p_Base-DSS1Default_Network_HD.res1d\n",
      "process NSSA_GA_EX-5y-24h-AES_2030p_Base-DSS2Default_Network_HD.res1d\n",
      "process NSSA_GA_EX-10y-24h-AES_2030p_Base-DSS3Default_Network_HD.res1d\n",
      "process NSSA_GA_EX-25y-24h-AES_2030p_Base-DSS10Default_Network_HD.res1d\n",
      "process LISA_DWF_2025pop_2025Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-2yr-24hr-SCS_2025pop_2025Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-5yr-24hr-SCS_2025pop_2025Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-10yr-24hr-SCS_2025pop_2025Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-25yr-24hr-SCS_2025pop_2025Default_Network_HD.res1d\n",
      "process LISA_DWF_2030pop_2030Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-2yr-24hr-SCS_2030pop_2030Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-5yr-24hr-SCS_2030pop_2030Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-10yr-24hr-SCS_2030pop_2030Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-25yr-24hr-SCS_2030pop_2030Default_Network_HD.res1d\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Permanent Cell 3\n",
    "#Import model results\n",
    "\n",
    "try:\n",
    "    \n",
    "    def q_v_not_absolute(val_list):\n",
    "        val_max = max(val_list)\n",
    "        val_min = min(val_list)\n",
    "        if abs(min(val_list)) > abs(max(val_list)):\n",
    "            val_max = min(val_list)\n",
    "            val_min = min(val_list, key=abs)\n",
    "        if max(val_list) * min(val_list) < 0:\n",
    "            val_min = np.nan\n",
    "        return [val_min,val_max]\n",
    "    \n",
    "    for m_index, m in enumerate(master_list):\n",
    "\n",
    "        model_area = m[0]\n",
    "        model = m[1]\n",
    "        model_folder = m[2]\n",
    "        output_folder = m[3]\n",
    "        result_list = m[4]\n",
    "\n",
    "        db_type = os.path.splitext(model)[1][1:]\n",
    "\n",
    "        node_df = pd.read_csv(model_manhole_csv, dtype={'facilityid': str,'muid': str})\n",
    "        node_df.rename(columns={'sewer_area':'SewerageArea','facilityid':'FacilityID','muid':'ModelID','match_code':'Match_Code','acronym':'Acronym','mhname':'MHName','soh':'SOH','invertlevel':'InvertLevel'},inplace=True)\n",
    "        node_df = node_df[['SewerageArea','FacilityID','Acronym','MHName','ModelID','InvertLevel','SOH','Match_Code']]\n",
    "        node_df = node_df[node_df.SewerageArea==model_area]\n",
    "\n",
    "        pipe_df = pd.read_csv(model_pipe_csv, dtype={'facilityid': str,'muid': str})\n",
    "        pipe_df.rename(columns={'sewer_area':'SewerageArea','facilityid':'FacilityID','muid':'ModelID','match_code':'Match_Code','acronym':'Acronym'},inplace=True)\n",
    "        pipe_df = pipe_df[['SewerageArea','FacilityID','Acronym','ModelID','Match_Code']]\n",
    "        pipe_df = pipe_df[pipe_df.SewerageArea==model_area]\n",
    "\n",
    "        for r in result_list:\n",
    "            description = r[0]\n",
    "            pop_year = r[1]\n",
    "            result_file = r[2]\n",
    "            #If mdb (VSA), the results are in the same folder as the model\n",
    "            if db_type.lower() == 'mdb':\n",
    "                result_path = model_folder + '\\\\' + result_file\n",
    "                if not os.path.exists(result_path):\n",
    "                    #This will end the script execution with an error.\n",
    "                    raise ValueError(\"The following result file was not found in the model folder: \" + result_file)     \n",
    "            #If sqlite (MIKE+: FSA, NSSA, LISA), then the results are found in sub folders auto-named after the model.\n",
    "            elif db_type.lower() == 'sqlite':\n",
    "                file_found = False\n",
    "                for f1 in os.listdir(model_folder):\n",
    "                    if f1[-7:] == '.sqlite':\n",
    "                        #browse subfolder\n",
    "                        result_subfolder = os.path.basename(f1)[:-7] + '_m1d - Result Files'\n",
    "                        for f2 in os.listdir(model_folder + '\\\\' + result_subfolder):\n",
    "                            if os.path.basename(f2) == result_file:\n",
    "                                result_path = model_folder + '\\\\' + result_subfolder + '\\\\' + f2\n",
    "                                file_found = True\n",
    "                if not file_found:\n",
    "                    #This will end the script execution with an error.\n",
    "                    raise ValueError(\"The following result file was not found: \" + result_file)\n",
    "            else:\n",
    "                #This will end the script execution with an error.\n",
    "                raise ValueError(\"The variable 'db_type' must be 'mdb' or 'sqlite'.\")\n",
    "\n",
    "            res1d = Res1D(result_path)\n",
    "            print('process ' + result_file)\n",
    "            sim_start = res1d.time_index.min()\n",
    "            start = sim_start + timedelta(days=1)\n",
    "            end = res1d.time_index.max()\n",
    "            sim_seconds = (end - sim_start).total_seconds()\n",
    "            #Number of timesteps\n",
    "            timesteps = len(res1d.time_index)-1\n",
    "            #Number of seconds in one timestep\n",
    "            timestep_seconds = sim_seconds / timesteps\n",
    "            #Number of timesteps in one day (Used to crop first day and for ADWF only include last day)\n",
    "            one_day_steps = int(86400 / timestep_seconds)\n",
    "\n",
    "            #@@@@@@@@@@@@@@@@@@@@@HGL\n",
    "            if description.lower() == 'dwf':\n",
    "                #Build column names and prefill with empty (nan) values.\n",
    "                hgl_name = 'PDWF_' + str(pop_year) + '_HGL'\n",
    "                hgl_avg_name = 'ADWF_' + str(pop_year) + '_HGL'\n",
    "                node_df[hgl_avg_name] = np.nan\n",
    "                hgl_min_name = 'MDWF_' + str(pop_year) + '_HGL'\n",
    "                node_df[hgl_min_name] = np.nan\n",
    "            else:\n",
    "                hgl_name = description + '_' + str(pop_year) + '_HGL'\n",
    "            node_df[hgl_name] = np.nan\n",
    "            nodes = [node.Id for node in res1d.data.Nodes]\n",
    "            for index, row in node_df.iterrows():\n",
    "                muid = row['ModelID']\n",
    "                if muid in nodes:\n",
    "                    #List hgl values with first day removed.\n",
    "                    hgl = max(list(res1d.query.GetNodeValues(muid, \"WaterLevel\"))[one_day_steps:])\n",
    "                    node_df.loc[index,hgl_name] = hgl\n",
    "                    if description.lower() == 'dwf':\n",
    "                        #For average, use only last day to calculate average\n",
    "                        hgl_avg = sum(list(res1d.query.GetNodeValues(muid, \"WaterLevel\"))[-one_day_steps:])/one_day_steps\n",
    "                        node_df.loc[index,hgl_avg_name] = hgl_avg\n",
    "                        hgl_min = min(list(res1d.query.GetNodeValues(muid, \"WaterLevel\"))[one_day_steps:])\n",
    "                        node_df.loc[index,hgl_min_name] = hgl_min\n",
    "\n",
    "\n",
    "            #@@@@@@@@@@@@@@@@@@@@@ Q and V\n",
    "            #The pipe muids are given a suffix (hyphen and incrementing counter) in mikeio1d which is removed here\n",
    "            pipes = [pipe.Id[:pipe.Id.rfind('-')] for pipe in res1d.data.Reaches]\n",
    "            if description.lower() == 'dwf':\n",
    "                #Build column names and prefill with empty (nan) values.\n",
    "                q_name = 'PDWF_' + str(pop_year) + '_Q'\n",
    "                q_avg_name = 'ADWF_' + str(pop_year) + '_Q'\n",
    "                pipe_df[q_avg_name] = np.nan           \n",
    "                q_min_name = 'MDWF_' + str(pop_year) + '_Q'\n",
    "                pipe_df[q_min_name] = np.nan\n",
    "\n",
    "                v_name = 'PDWF_' + str(pop_year) + '_V'\n",
    "                v_avg_name = 'ADWF_' + str(pop_year) + '_V'\n",
    "                pipe_df[v_avg_name] = np.nan\n",
    "                v_min_name = 'MDWF_' + str(pop_year) + '_V'\n",
    "                pipe_df[v_min_name] = np.nan\n",
    "\n",
    "            else:\n",
    "                q_name = description + '_' + str(pop_year) + '_Q'\n",
    "                v_name = description + '_' + str(pop_year) + '_V'\n",
    "            pipe_df[q_name] = np.nan\n",
    "            pipe_df[v_name] = np.nan\n",
    "            for index, row in pipe_df.iterrows():\n",
    "                muid = row['ModelID']\n",
    "                if muid in pipes:\n",
    "                    #List of flow values with first day cropped/\n",
    "                    q_list = list(res1d.query.GetReachStartValues(muid, \"Discharge\"))[one_day_steps:]\n",
    "                    #Convert to L/s\n",
    "                    q_list = [q*1000 for q in q_list]\n",
    "                    v_list = list(res1d.query.GetReachStartValues(muid, \"FlowVelocity\"))[one_day_steps:]\n",
    "\n",
    "                    if absolute_velocity_discharge:\n",
    "                        #Get the absolute maxes and mins (every value absolute)\n",
    "                        q_max = max([abs(q) for q in q_list])\n",
    "                        v_max = max([abs(v) for v in v_list])\n",
    "                        q_min = min([abs(q) for q in q_list])\n",
    "                        v_min = min([abs(v) for v in v_list])\n",
    "\n",
    "                    else:\n",
    "                        q_stats = q_v_not_absolute(q_list)\n",
    "                        q_max = q_stats[1]\n",
    "                        q_min = q_stats[0]\n",
    "                     \n",
    "                        v_stats = q_v_not_absolute(v_list)\n",
    "                        v_max = v_stats[1]  \n",
    "                        v_min = v_stats[0]\n",
    "                        \n",
    "                    pipe_df.loc[index,q_name] = q_max\n",
    "                    pipe_df.loc[index,v_name] = v_max\n",
    "\n",
    "                    if description.lower() == 'dwf':\n",
    "                        pipe_df.loc[index,q_min_name] = q_min\n",
    "                        pipe_df.loc[index,v_min_name] = v_min\n",
    "\n",
    "                        q_avg = sum(list(res1d.query.GetReachStartValues(muid, \"Discharge\"))[-one_day_steps:])/one_day_steps*1000\n",
    "                        if absolute_velocity_discharge:\n",
    "                            #Only take the absolute of the final average, not every timestep, otherwise it will be skewed.\n",
    "                            q_avg = abs(q_avg)\n",
    "                        pipe_df.loc[index,q_avg_name] = q_avg\n",
    "\n",
    "                        v_avg = sum(list(res1d.query.GetReachStartValues(muid, \"FlowVelocity\"))[-one_day_steps:])/one_day_steps\n",
    "                        if absolute_velocity_discharge:\n",
    "                            #Only take the absolute of the final average, not every timestep, otherwise it will be skewed.\n",
    "                            v_avg = abs(v_avg)\n",
    "                        pipe_df.loc[index,v_avg_name] = v_avg\n",
    "\n",
    "\n",
    "        if m_index == 0:\n",
    "            #Create the table with all values\n",
    "            node_df_all = node_df.copy()\n",
    "            pipe_df_all = pipe_df.copy()\n",
    "        else:\n",
    "            #Append to the table with all value (alread created at m_index == 0)\n",
    "            node_df_all = pd.concat([node_df_all,node_df])\n",
    "            pipe_df_all = pd.concat([pipe_df_all,pipe_df])\n",
    "\n",
    "    if len(acronym_filter) > 0:\n",
    "        #If only some acronyms are to be created\n",
    "        node_df_all = node_df_all[node_df_all.Acronym.isin(acronym_filter)]\n",
    "        pipe_df_all = pipe_df_all[pipe_df_all.Acronym.isin(acronym_filter)]\n",
    "\n",
    "    node_df_all.drop(columns=['Match_Code'],inplace=True)\n",
    "    pipe_df_all.drop(columns=['Match_Code'],inplace=True)\n",
    "    \n",
    "    #Replace column name hyphens with underscores as requested by GIS\n",
    "    node_df_all.rename(columns=lambda x: x.replace('-', '_'), inplace=True)\n",
    "    pipe_df_all.rename(columns=lambda x: x.replace('-', '_'), inplace=True)\n",
    "    \n",
    "    #Remove duplicates\n",
    "    node_duplicates = node_df_all[node_df_all.duplicated(subset=['FacilityID'], keep=False)]\n",
    "    node_duplicates = set(node_duplicates['FacilityID'])\n",
    "    node_duplicates = ','.join(map(str, node_duplicates))\n",
    "    node_df_all = node_df_all.drop_duplicates(subset=['FacilityID'], keep='first')\n",
    "        \n",
    "    pipe_duplicates = pipe_df_all[pipe_df_all.duplicated(subset=['FacilityID'], keep=False)]\n",
    "    pipe_duplicates = set(pipe_duplicates['FacilityID'])\n",
    "    pipe_duplicates = ','.join(map(str, pipe_duplicates))\n",
    "    pipe_df_all = pipe_df_all.drop_duplicates(subset=['FacilityID'], keep='first')\n",
    "    \n",
    "    #Create the csv files\n",
    "    node_df_all.to_csv(output_folder + '\\\\Model_Nodes.csv', index=False)   \n",
    "    pipe_df_all.to_csv(output_folder + '\\\\Model_Pipes.csv', index=False) \n",
    "    print('Done')     \n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 3', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09f54e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PERMANENT CELL 4\n",
    "message = 'All cells ran successfully.\\n\\n'\n",
    "message += 'Removed duplicates:\\n\\n'\n",
    "message += f'RAWN: {rawn_duplicates}\\n'\n",
    "message += f'Model Nodes: {node_duplicates}\\n'\n",
    "message += f'Model Pipes: {pipe_duplicates}\\n'\n",
    "MessageBox(None,message.encode('utf-8'), b'Done', 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
